# Session 14: Interventions and Do-Calculus

*Reading: Pearl do-calculus intro (high level); Fritz notes sections about interventions*

---

## The Core Insight

**do(X=x) ≠ observing X=x**

- **Observing** X=x: We learn X=x, which tells us about the causes of X
- **Intervening** do(X=x): We force X=x, breaking the causal mechanism that normally determines X

```text
Observing:     P(Y | X=x)      ← What is Y when we see X=x?
Intervening:   P(Y | do(X=x))  ← What is Y when we set X=x?
```

---

## The Graphical Operation

### Intervention = Cut Incoming Edges

When we intervene do(X=x):
1. **Remove** all edges into X (cut the causal mechanism)
2. **Replace** P(X|parents) with a point mass at x

```text
Original graph:              After do(X=x):

    Z                           Z
    ↓
    X → Y                       X=x → Y

P(X,Y,Z) = P(Z)·P(X|Z)·P(Y|X)   P(Y|do(X=x)) uses P(Y|X) only
```

### In Code

```rust
use compositional_prob::{BayesNet, Dist, Kernel};

impl BayesNet {
    /// Create a new network with intervention do(var=value).
    ///
    /// This "mutilates" the graph by:
    /// 1. Removing the factor for the intervened variable
    /// 2. Adding a point mass at the intervention value
    pub fn intervene(&self, var: usize, value: usize) -> BayesNet {
        let mut new_net = BayesNet::new(self.var_states.clone());
        new_net.var_names = self.var_names.clone();

        for factor in &self.factors {
            if factor.variable == var {
                // Replace with point mass at intervention value
                let point_mass = Dist::point(self.var_states[var], value);
                new_net.add_prior(var, &point_mass).unwrap();
            } else {
                // Keep other factors unchanged
                new_net.add_factor(factor.clone()).unwrap();
            }
        }

        new_net
    }
}
```

---

## Causal vs Observational: A Confounding Example

### The Setup: Smoking, Tar, and Cancer

```text
           Genotype (G)
           ↙        ↘
      Smoking (S) → Tar (T) → Cancer (C)
```

- G: genetic predisposition (affects both smoking tendency and cancer risk)
- S: smoking behavior
- T: tar deposits in lungs
- C: lung cancer

### The Confounding Problem

**Observational**: People who smoke more have higher cancer rates.

But is this because:
1. Smoking causes cancer (causal effect)?
2. Genes cause both smoking and cancer (confounding)?

```text
P(C | S=1) ≠ P(C | do(S=1))

Observational: P(C|S=1) includes effect of G
Interventional: P(C|do(S=1)) isolates causal effect of S
```

### Computing Both

```rust
use compositional_prob::{BayesNet, Dist, Kernel};
use std::collections::HashMap;

fn smoking_example() {
    // Build the causal graph
    let mut net = BayesNet::new(vec![2, 2, 2, 2]);  // G, S, T, C
    let net = net.with_names(vec!["Genotype", "Smoking", "Tar", "Cancer"]);
    let mut net = net;

    // P(G) - genetic predisposition
    net.add_prior(0, &Dist::new(vec![0.7, 0.3]).unwrap()).unwrap();

    // P(S|G) - smoking depends on genes
    net.add_conditional(1, vec![0], Kernel::new(vec![
        vec![0.8, 0.2],  // G=0: 20% smoke
        vec![0.3, 0.7],  // G=1: 70% smoke
    ]).unwrap()).unwrap();

    // P(T|S) - tar depends on smoking
    net.add_conditional(2, vec![1], Kernel::new(vec![
        vec![0.95, 0.05],  // S=0: 5% tar
        vec![0.2, 0.8],    // S=1: 80% tar
    ]).unwrap()).unwrap();

    // P(C|G,T) - cancer depends on genes AND tar
    net.add_conditional(3, vec![0, 2], Kernel::new(vec![
        vec![0.98, 0.02],  // G=0,T=0: 2% cancer
        vec![0.85, 0.15],  // G=0,T=1: 15% cancer
        vec![0.90, 0.10],  // G=1,T=0: 10% cancer
        vec![0.60, 0.40],  // G=1,T=1: 40% cancer
    ]).unwrap()).unwrap();

    // OBSERVATIONAL: P(Cancer | Smoking=1)
    let mut evidence = HashMap::new();
    evidence.insert(1, 1);  // Observe Smoking=1
    let p_cancer_obs = net.query(3, &evidence).unwrap();

    // INTERVENTIONAL: P(Cancer | do(Smoking=1))
    let intervened_net = net.intervene(1, 1);  // Force Smoking=1
    let p_cancer_int = intervened_net.marginal(3).unwrap();

    println!("Observational P(Cancer | Smoking=1):");
    println!("  [{:.4}, {:.4}]", p_cancer_obs.p[0], p_cancer_obs.p[1]);

    println!("Interventional P(Cancer | do(Smoking=1)):");
    println!("  [{:.4}, {:.4}]", p_cancer_int.p[0], p_cancer_int.p[1]);

    // The difference reveals the confounding!
}
```

---

## Why The Difference Matters

### Observational P(C|S=1)

When we observe someone smokes:
- We learn they're more likely to have the "smoking gene"
- The gene also increases cancer risk independently
- So observed smokers have higher cancer even beyond smoking's direct effect

```text
P(C=1|S=1) = Σ_G,T P(C=1|G,T) · P(G|S=1) · P(T|S=1)
                                ^^^^^^^^
                                This is affected by S=1!
```

### Interventional P(C|do(S=1))

When we force everyone to smoke:
- Genetic distribution is unchanged (we didn't change genes)
- Only the direct causal path S→T→C matters

```text
P(C=1|do(S=1)) = Σ_G,T P(C=1|G,T) · P(G) · P(T|S=1)
                                    ^^^^
                                    Original distribution!
```

---

## The Adjustment Formula

### When Confounders Are Observed

If Z is a set of variables that blocks all "backdoor paths" from X to Y:

```text
P(Y | do(X=x)) = Σ_z P(Y | X=x, Z=z) · P(Z=z)
```

This is the **backdoor adjustment formula**.

```rust
/// Compute P(Y | do(X=x)) using backdoor adjustment.
///
/// adjustment_set: variables Z that block backdoor paths
pub fn backdoor_adjustment(
    net: &BayesNet,
    x_var: usize,
    x_val: usize,
    y_var: usize,
    adjustment_set: &[usize],
) -> Dist {
    let n_y_states = net.var_states[y_var];
    let mut result = vec![0.0; n_y_states];

    // Enumerate all values of adjustment variables
    let adj_states: Vec<usize> = adjustment_set
        .iter()
        .map(|&v| net.var_states[v])
        .collect();
    let total_adj: usize = adj_states.iter().product();

    for adj_idx in 0..total_adj {
        // Decode adjustment values
        let mut adj_values = vec![0; adjustment_set.len()];
        let mut idx = adj_idx;
        for i in (0..adjustment_set.len()).rev() {
            adj_values[i] = idx % adj_states[i];
            idx /= adj_states[i];
        }

        // Build evidence: X=x, Z=z
        let mut evidence = HashMap::new();
        evidence.insert(x_var, x_val);
        for (i, &z) in adjustment_set.iter().enumerate() {
            evidence.insert(z, adj_values[i]);
        }

        // P(Y | X=x, Z=z)
        let p_y_given_xz = net.query(y_var, &evidence).unwrap();

        // P(Z=z)
        let mut z_evidence = HashMap::new();
        for (i, &z) in adjustment_set.iter().enumerate() {
            z_evidence.insert(z, adj_values[i]);
        }
        let p_z = compute_evidence_prob(net, &z_evidence);

        // Accumulate
        for y in 0..n_y_states {
            result[y] += p_y_given_xz.p[y] * p_z;
        }
    }

    Dist::from_weights(result).unwrap()
}
```

---

## The Three Rules of Do-Calculus

Pearl's do-calculus provides rules for manipulating interventional distributions:

### Rule 1: Insertion/Deletion of Observations

```text
P(Y | do(X), Z, W) = P(Y | do(X), W)
```
if Z is independent of Y given X and W in the manipulated graph.

### Rule 2: Action/Observation Exchange

```text
P(Y | do(X), do(Z), W) = P(Y | do(X), Z, W)
```
if Z is independent of Y given X and W when we remove arrows into Z.

### Rule 3: Insertion/Deletion of Actions

```text
P(Y | do(X), do(Z), W) = P(Y | do(X), W)
```
if Z is independent of Y given X and W when we remove arrows into Z and certain other conditions hold.

These rules are **complete**: if a causal effect can be computed from observations, do-calculus can derive it.

---

## Categorical Perspective

### Interventions as Graph Surgery

In categorical terms, an intervention:
1. **Removes** a morphism (the mechanism for X)
2. **Replaces** it with a constant morphism (point mass)

```text
Original:     1 → Z → X → Y
                    ↘   ↗

do(X=x):      1 → Z     X=x → Y
                  (no arrow to X)
```

### Causal Models as Structured Categories

A **structural causal model** can be viewed as:
- Objects: Random variables
- Morphisms: Causal mechanisms (kernels)
- Structure: The DAG constrains which morphisms exist

Interventions are **functors** that modify this structure!

---

## Summary

| Concept | Meaning | Graph Operation |
|---------|---------|-----------------|
| P(Y\|X=x) | Observational | No change |
| P(Y\|do(X=x)) | Interventional | Cut edges into X |
| Confounding | Common cause of X and Y | Backdoor path |
| Adjustment | Σ_z P(Y\|X,Z)·P(Z) | Block backdoor |
| do-calculus | Rules for computing do | Graph criteria |

**Key insight**: Observing vs intervening are fundamentally different operations. Observing X=x updates our beliefs about X's causes. Intervening do(X=x) breaks the causal mechanism, isolating X's effect on downstream variables.

---

## Next: Session 15

With causal interventions understood, Session 15 explores:
- **Counterfactuals**: "What would have happened if...?"
- **Potential outcomes** framework
- Connection to causal inference in practice
