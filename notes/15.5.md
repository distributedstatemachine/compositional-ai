# Session 15.5: Tensor Semantics — From Grammar to Vectors

## Overview

This session connects pregroup grammars (Session 15) with vector spaces via a **semantics functor**. This is the core of DisCoCat (Distributional Compositional Categorical) models: we assign vectors and tensors to words, then compute sentence meaning by tensor contraction following the grammatical structure.

**Key insight**: The parse structure tells us HOW to contract tensors!

## The Semantics Functor

Recall from Session 3: a **functor** is a structure-preserving map between categories.

The semantics functor maps:
```
F: Grammar → Vect
```

Where:
- **Grammar**: Pregroup category (types, reductions)
- **Vect**: Category of finite-dimensional vector spaces

### Object Mapping

| Grammar Type | Vector Space |
|--------------|--------------|
| N (noun) | ℝⁿ (noun space) |
| S (sentence) | ℝ or ℝᵐ (sentence space) |
| Nˡ, Nʳ | (ℝⁿ)* ≅ ℝⁿ (dual space) |
| Sˡ, Sʳ | ℝ* ≅ ℝ |

For finite-dimensional spaces, V* ≅ V, so we can treat adjoints as the same space.

### Morphism Mapping

Reductions (cups) map to tensor contractions:
- `N · Nʳ → 1` becomes inner product (contract indices)
- `Nˡ · N → 1` becomes inner product (contract indices)

## Word Meanings as Tensors

### Nouns → Vectors

A noun is a vector in noun space ℝⁿ:

```
Alice ↦ [0.8, 0.6]     (e.g., "human-like", "female-like")
Bob   ↦ [0.7, 0.3]     (e.g., "human-like", "male-like")
dog   ↦ [0.2, 0.9]     (e.g., "animal-like", "loyal")
```

These vectors come from **distributional semantics** — words that appear in similar contexts have similar vectors.

### Transitive Verbs → Matrices

A transitive verb `Nʳ · S · Nˡ` maps to a tensor in:
```
ℝⁿ ⊗ ℝ ⊗ ℝⁿ ≅ ℝⁿˣⁿ (for S = ℝ)
```

This is just a matrix! The verb "loves" might be:
```
        object →
      ┌         ┐
loves = │ 0.9  0.1 │  subject
      │ 0.2  0.8 │     ↓
      └         ┘
```

High value at (i,j) means "subject type i loves object type j".

### Intransitive Verbs → Vectors

An intransitive verb `Nʳ · S` is a vector in ℝⁿ:
```
runs   ↦ [0.5, 0.8]   (animals run more than humans?)
sleeps ↦ [0.9, 0.7]   (everyone sleeps)
```

### Adjectives → Matrices

An adjective `N · Nˡ` is a linear map ℝⁿ → ℝⁿ (a matrix):
```
big ↦ [[1.2, 0], [0, 1.0]]   (scales the first component)
```

## Sentence Meaning via Contraction

### Example: "Alice runs"

```
Parse:
  Alice : N    →  v ∈ ℝⁿ
  runs  : Nʳ·S →  w ∈ ℝⁿ

  N · Nʳ · S
  └──┬──┘
     ↓ contract
     S

Computation:
  ⟨v, w⟩ = Σᵢ vᵢ · wᵢ = scalar (sentence meaning)
```

If Alice = [0.8, 0.6] and runs = [0.5, 0.8]:
```
meaning("Alice runs") = 0.8×0.5 + 0.6×0.8 = 0.4 + 0.48 = 0.88
```

### Example: "Alice loves Bob"

```
Parse:
  Alice : N       →  v ∈ ℝⁿ
  loves : Nʳ·S·Nˡ →  M ∈ ℝⁿˣⁿ
  Bob   : N       →  w ∈ ℝⁿ

  N · Nʳ · S · Nˡ · N
  └──┬──┘     └──┬──┘
     ↓           ↓
     contract   contract

Computation:
  vᵀ · M · w = Σᵢⱼ vᵢ · Mᵢⱼ · wⱼ = scalar
```

This is matrix-vector multiplication:
```
meaning("Alice loves Bob") = Alice · loves · Bob
                           = [0.8, 0.6] · [[0.9, 0.1], [0.2, 0.8]] · [0.7, 0.3]
                           = [0.84, 0.56] · [0.7, 0.3]
                           = 0.588 + 0.168 = 0.756
```

### Example: "big dog"

```
Parse:
  big : N·Nˡ  →  A ∈ ℝⁿˣⁿ (linear map)
  dog : N     →  v ∈ ℝⁿ

  N · Nˡ · N
      └──┬──┘
         ↓ contract

Computation:
  A · v = modified noun vector
```

The adjective **transforms** the noun vector.

## The Functor Laws

The semantics functor must preserve:

**Identity**: `F(id_X) = id_{F(X)}`
- The identity reduction maps to the identity linear map

**Composition**: `F(g ∘ f) = F(g) ∘ F(f)`
- Sequential reductions = sequential contractions

This is what makes the semantics **compositional** — the meaning of the whole is systematically determined by the meanings of the parts!

## Cups and Caps in Vect

In the category Vect, cups and caps are:

**Cup** (evaluation): `V ⊗ V* → ℝ`
```
cup(v, f) = f(v) = ⟨v, w⟩  (inner product when V* ≅ V)
```

**Cap** (coevaluation): `ℝ → V* ⊗ V`
```
cap(1) = Σᵢ eᵢ ⊗ eᵢ  (sum of basis elements)
```

These satisfy the snake equations:
```
(cup ⊗ id) ∘ (id ⊗ cap) = id
(id ⊗ cup) ∘ (cap ⊗ id) = id
```

## String Diagrams = Tensor Networks

The pregroup parse gives us a **string diagram**, which IS a **tensor network**:

```
Alice    loves       Bob
  │        │          │
  ↓        ↓          ↓
 [v]      [M]        [w]
  │      /   \        │
  │     /     \       │
  └────┘       └──────┘
       ↓           ↓
    contract    contract
         \       /
          \     /
           \   /
            ↓
         scalar
```

This is exactly how tensor network contraction works!

## Connection to Session 3

Remember the `SemanticsFunctorMarker` we defined in Session 3?

```rust
/// Marker: This will be used in Session 15 for grammar → Vect semantics
pub struct SemanticsFunctorMarker;
```

Now we're implementing it! The semantics functor:
1. Maps pregroup types to vector spaces
2. Maps words to vectors/tensors
3. Maps reductions to contractions
4. Preserves composition (functor laws)

## Distributional Semantics

Where do the word vectors come from?

**Distributional hypothesis**: "Words that occur in similar contexts have similar meanings."

Methods:
1. **Co-occurrence matrices**: Count word-word co-occurrences
2. **Word2Vec**: Neural network trained to predict context
3. **GloVe**: Matrix factorization on co-occurrences
4. **BERT embeddings**: Contextual representations

DisCoCat combines:
- **Distributional**: Word vectors from corpus statistics
- **Compositional**: Grammatical structure determines how to combine
- **Categorical**: Functorial semantics ensures coherence

## Implementation Sketch

```rust
/// Semantics functor: Grammar → Vect
pub struct Semantics {
    /// Dimension of noun space
    noun_dim: usize,
    /// Word meanings
    lexicon: HashMap<String, Tensor>,
}

impl Semantics {
    /// Map a pregroup type to a tensor shape
    pub fn type_to_shape(&self, typ: &PregroupType) -> Vec<usize> {
        typ.factors.iter().map(|atom| {
            match atom.base {
                BasicType::N => self.noun_dim,
                BasicType::S => 1,  // scalars
                _ => self.noun_dim,
            }
        }).collect()
    }

    /// Compute sentence meaning by contraction
    pub fn meaning(&self, parse: &ParseResult) -> f64 {
        // Get tensors for each word
        let tensors: Vec<_> = parse.typed_words.iter()
            .map(|tw| self.lexicon.get(&tw.word).unwrap())
            .collect();

        // Contract according to cups
        let cups = extract_cups(parse);
        contract_network(&tensors, &cups)
    }
}
```

## Exercises

1. **Compute meaning**: Given Alice = [1, 0], Bob = [0, 1], and loves = [[1, 0], [0, 1]] (identity), what is the meaning of "Alice loves Bob"?

2. **Symmetry**: With the identity matrix for "loves", show that meaning("Alice loves Bob") = meaning("Bob loves Alice"). What matrix breaks this symmetry?

3. **Adjective composition**: If big = [[2, 0], [0, 1]] and dog = [0.3, 0.9], compute the vector for "big dog".

4. **Intransitive**: Design vectors for "Alice", "Bob", and "runs" such that "Alice runs" > "Bob runs" (Alice is more of a runner).

## Summary

| Concept | Grammar | Vect |
|---------|---------|------|
| Noun | N | ℝⁿ |
| Sentence | S | ℝ |
| Adjoint | Nˡ, Nʳ | (ℝⁿ)* ≅ ℝⁿ |
| Noun word | N | vector ∈ ℝⁿ |
| Trans. verb | Nʳ·S·Nˡ | matrix ∈ ℝⁿˣⁿ |
| Intrans. verb | Nʳ·S | vector ∈ ℝⁿ |
| Reduction | cup | inner product |
| Parse | diagram | tensor network |
| Meaning | S | scalar |

## Reading

- Coecke, Sadrzadeh, Clark: "Mathematical Foundations for a Compositional Distributional Model of Meaning"
- Callback to Session 3: Functors and Natural Transformations
- Kartsaklis et al.: "A Study of Entanglement in a Categorical Framework of Natural Language"

## Next

Session 16: Full DisCoCat implementation — putting it all together with real word vectors and similarity measures.
