# Session 5: Monoidal Categories — The Algebra of Parallel and Sequential

*Reading: Fong & Spivak, Seven Sketches — monoidal categories + "wiring diagrams"*

---

## The Two Fundamental Operations

In Session 4, we built diagrams with boxes and wires. Now we formalize the **two ways to combine** diagrams:

| Operation | Symbol | Meaning | Code |
|-----------|--------|---------|------|
| Sequential | `;` or `∘` | "then" — connect end-to-end | `f.then(g)` |
| Parallel | `⊗` | "tensor" — side by side | `f.tensor(g)` |

These aren't just convenient — they form a **monoidal category**, a structure with precise laws.

---

## Sequential Composition: Categorical Composition

Sequential composition is the familiar categorical composition:

```
   f: A → B       g: B → C

   ┌───┐          ┌───┐              ┌───┐    ┌───┐
───│ f │───   ;  ─│ g │───    =   ───│ f │────│ g │───
   └───┘          └───┘              └───┘    └───┘

                                     f ; g : A → C
```

**Key property**: The output type of `f` must match the input type of `g`.

In code:
```rust
// f: scalar → vector(10)
// g: vector(10) → vector(20)
let fg = f.then(g)?;  // fg: scalar → vector(20)
```

### The Laws of Sequential Composition

```
Associativity:    (f ; g) ; h  =  f ; (g ; h)
Identity:         id_A ; f  =  f  =  f ; id_A
```

These say:
1. Grouping doesn't matter — just wire them in order
2. Identity morphisms do nothing

---

## Parallel Composition: The Monoidal Product (⊗)

Parallel composition puts things **side by side** with no connection:

```
   f: A → B

   ┌───┐                         ┌───┐
───│ f │───                   ───│ f │───
   └───┘           ⊗             └───┘
   ┌───┐                         ┌───┐
───│ g │───                   ───│ g │───
   └───┘                         └───┘

   g: C → D                   f ⊗ g : (A, C) → (B, D)
```

**Key property**: No type matching required — they're independent.

In code:
```rust
// f: scalar → vector(10)
// g: vector(20) → vector(30)
let fg = f.tensor(g);  // fg: (scalar, vector(20)) → (vector(10), vector(30))
```

### The Laws of Parallel Composition

```
Associativity:    (f ⊗ g) ⊗ h  ≅  f ⊗ (g ⊗ h)
Unit:             I ⊗ f  ≅  f  ≅  f ⊗ I
```

Where `I` is the **monoidal unit** (empty diagram / unit type).

---

## The Interchange Law: Where It Gets Interesting

Here's the deep insight. Suppose we have four morphisms:

```
f₁: A → B    g₁: B → C
f₂: X → Y    g₂: Y → Z
```

We can combine them two ways:

### Way 1: Compose first, then tensor

```
(f₁ ; g₁) ⊗ (f₂ ; g₂)

   ┌────┐    ┌────┐
───│ f₁ │────│ g₁ │───
   └────┘    └────┘
                        ← These are independent
   ┌────┐    ┌────┐
───│ f₂ │────│ g₂ │───
   └────┘    └────┘
```

### Way 2: Tensor first, then compose

```
(f₁ ⊗ f₂) ; (g₁ ⊗ g₂)

   ┌────┐         ┌────┐
───│ f₁ │─────────│ g₁ │───
   └────┘         └────┘
                            ← Same result!
   ┌────┐         ┌────┐
───│ f₂ │─────────│ g₂ │───
   └────┘         └────┘
```

### The Interchange Law

```
(f₁ ; g₁) ⊗ (f₂ ; g₂)  =  (f₁ ⊗ f₂) ; (g₁ ⊗ g₂)
```

**In words**: "Compose then tensor" equals "tensor then compose".

---

## Interchange Law Intuition

Why does this work? Think of it as a **grid**:

```
        Time →

    ┌────────────────────┐
    │   f₁    │    g₁    │  ← Track 1
  S ├─────────┼──────────┤
  p │   f₂    │    g₂    │  ← Track 2
  a └────────────────────┘
  c       t=0      t=1
  e
  ↓
```

The interchange law says: **it doesn't matter whether you think of this as**:
- Two parallel tracks, each running its own sequence, OR
- Two time slices, each containing parallel operations

Both perspectives describe the same computation!

### Real-World Analogy

Imagine two assembly lines in a factory:

```
Line 1:  [Cut] → [Paint]
Line 2:  [Mold] → [Polish]
```

**Interpretation 1** (compose then tensor):
- Line 1 does cut-then-paint
- Line 2 does mold-then-polish
- Run both lines simultaneously

**Interpretation 2** (tensor then compose):
- First phase: cut and mold happen together
- Second phase: paint and polish happen together

Same end result! The products come out the same.

---

## Why This Matters for AI Systems

Consider a multi-agent pipeline:

```
                ┌───────────┐    ┌──────────┐
   query ───────│ Research  │────│ Summarize│──── summary
                └───────────┘    └──────────┘

                ┌───────────┐    ┌──────────┐
   data ────────│ Analyze   │────│ Visualize│──── charts
                └───────────┘    └──────────┘
```

The interchange law tells us we can:
1. **Pipeline-think**: Each track is a complete pipeline
2. **Stage-think**: Each column is a parallel batch

For optimization, stage-think might be better (batch similar operations).
For reasoning, pipeline-think might be clearer (each track is independent).

**The category theory guarantees they're equivalent.**

---

## The Monoidal Category Axioms (Complete)

A **monoidal category** (C, ⊗, I) has:

### Objects and Morphisms
- Objects (types): A, B, C, ...
- Morphisms (operations): f: A → B

### Sequential Composition (;)
- `f: A → B` and `g: B → C` gives `f ; g: A → C`
- Associative: `(f ; g) ; h = f ; (g ; h)`
- Identity: `id_A ; f = f = f ; id_A`

### Parallel Composition (⊗)
- `f: A → B` and `g: C → D` gives `f ⊗ g: A ⊗ C → B ⊗ D`
- Associative (up to isomorphism): `(A ⊗ B) ⊗ C ≅ A ⊗ (B ⊗ C)`
- Unit (up to isomorphism): `I ⊗ A ≅ A ≅ A ⊗ I`

### Interchange
- `(f₁ ; g₁) ⊗ (f₂ ; g₂) = (f₁ ⊗ f₂) ; (g₁ ⊗ g₂)`

### Coherence (the "it just works" laws)
- The associators and unitors satisfy pentagon and triangle identities
- (We won't prove these — they ensure all ways of re-bracketing give the same answer)

---

## In the Codebase

From `crates/core/src/diagram.rs`:

```rust
impl<O: Clone> Diagram<O> {
    /// Sequential composition: self ; other
    /// Connects self's outputs to other's inputs
    pub fn then(self, other: Self) -> Result<Self, CoreError> {
        // Verify: self.outputs.len() == other.inputs.len()
        // Verify: shapes are compatible
        // Connect and return composed diagram
    }

    /// Parallel composition: self ⊗ other
    /// Places diagrams side by side with no connections
    pub fn tensor(self, other: Self) -> Self {
        // Combine nodes from both
        // Concatenate inputs: [...self.inputs, ...other.inputs]
        // Concatenate outputs: [...self.outputs, ...other.outputs]
        // No new edges between them
    }
}
```

---

## Parallel Agents: Runtime Interpretation of Tensor

The diagram's `tensor` operation represents parallel structure *statically*. But we also need to *execute* parallel agents at runtime. This is where `ParallelAgents` comes in.

### The Agent Trait

```rust
/// Minimal interface for parallel execution
pub trait Agent: Clone + Send + Sync {
    type Input: Send;
    type Output: Send;
    type Error: Send;

    fn run(&self, input: Self::Input)
        -> impl Future<Output = Result<Self::Output, Self::Error>> + Send;
}
```

### ParallelAgents: The Runtime Tensor

```rust
/// A parallel executor that runs multiple agents concurrently
/// This is the runtime interpretation of the tensor product (⊗)
pub struct ParallelAgents<A> {
    agents: Vec<A>,
}

impl<A> ParallelAgents<A> {
    pub fn new() -> Self {
        Self { agents: Vec::new() }
    }

    pub fn with(mut self, agent: A) -> Self {
        self.agents.push(agent);
        self
    }

    /// Tensor two parallel groups
    pub fn tensor(mut self, mut other: Self) -> Self {
        self.agents.append(&mut other.agents);
        self
    }
}
```

### Execution: run_all and fan_out

```rust
impl<A, I, O, E> ParallelAgents<A>
where
    A: Agent<Input = I, Output = O, Error = E> + Send + 'static,
    I: Clone + Send + 'static,
    O: Send + 'static,
    E: Send + 'static,
{
    /// Execute all agents in parallel, collecting results
    pub async fn run_all(&self, inputs: Vec<I>) -> Result<Vec<O>, E> {
        let mut set = JoinSet::new();
        for (agent, input) in self.agents.iter().zip(inputs) {
            let agent = agent.clone();
            set.spawn(async move { agent.run(input).await });
        }
        // Collect results as they complete
        let mut results = Vec::new();
        while let Some(result) = set.join_next().await {
            results.push(result.unwrap()?);
        }
        Ok(results)
    }

    /// Fan-out: broadcast same input to all agents
    pub async fn fan_out(&self, input: I) -> Result<Vec<O>, E> {
        let inputs = vec![input; self.agents.len()];
        self.run_all(inputs).await
    }
}
```

### Fan-in: Combining Results

After parallel execution, we need to merge results. The `Combiner` trait handles this:

```rust
pub trait Combiner<T> {
    type Output;
    fn combine(&self, results: Vec<T>) -> Self::Output;
}

/// Concatenate string results
pub struct Concat;
impl Combiner<String> for Concat {
    type Output = String;
    fn combine(&self, results: Vec<String>) -> String {
        results.join("\n")
    }
}

/// Majority voting
pub struct Vote;
impl<T: Eq + Hash + Clone> Combiner<T> for Vote {
    type Output = Option<T>;
    fn combine(&self, results: Vec<T>) -> Option<T> {
        let mut counts: HashMap<T, usize> = HashMap::new();
        for r in results {
            *counts.entry(r).or_insert(0) += 1;
        }
        counts.into_iter().max_by_key(|(_, c)| *c).map(|(v, _)| v)
    }
}
```

---

## Complete Pattern: Fan-out → Process → Fan-in

The full parallel agent pattern combines tensor (fan-out), parallel execution, and sequential composition (fan-in):

```text
              ┌─────────────────┐
   query ─────│ WebSearch(tech) │─────┐
              └─────────────────┘     │    ┌───────────┐
                                      ├────│ Synthesis │──── report
              ┌─────────────────┐     │    └───────────┘
   query ─────│ WebSearch(acad) │─────┘
              └─────────────────┘

   Diagram: (search_tech ⊗ search_acad) ; synthesize
```

In code:

```rust
// Build parallel researchers
let researchers = ParallelAgents::new()
    .with(WebSearchAgent::new("technical"))
    .with(WebSearchAgent::new("academic"));

// Fan-out: same query to both agents
let findings = researchers.fan_out(query).await?;

// Sequential composition: feed parallel results to synthesis
let synthesis = SynthesisAgent::new();
let report = synthesis.run(findings).await?;
```

### Why This Matters

| Static (Diagram) | Dynamic (Runtime) |
|------------------|-------------------|
| `f.tensor(g)` | `ParallelAgents::new().with(f).with(g)` |
| Side-by-side structure | Concurrent execution |
| Type checking at compile time | Actual parallelism via tokio |
| `f.then(g)` | Sequential `await` |

The category theory (diagrams, tensor, composition) gives us the *structure*. The runtime (`ParallelAgents`, `JoinSet`) gives us the *execution*. They mirror each other:

- **Tensor** (`⊗`) in diagrams → **fan-out** in runtime
- **Sequential** (`;`) in diagrams → **await chain** in runtime
- **Interchange law** → **optimization freedom** (reorganize without changing semantics)

---

## Testing the Interchange Law

The tests verify the interchange law:

```rust
#[test]
fn test_interchange_law() {
    // (f1 ; g1) ⊗ (f2 ; g2) should equal (f1 ⊗ f2) ; (g1 ⊗ g2)
    let left = f1.clone().then(g1.clone())?.tensor(f2.clone().then(g2.clone())?);
    let right = f1.tensor(f2).then(g1.tensor(g2))?;

    // Same structure (node count, edge count, boundary sizes)
    assert_eq!(left.node_count(), right.node_count());
}
```

---

## Intuition Summary

| Concept | Intuition |
|---------|-----------|
| Sequential (`;`) | "Do this, then that" — time flows left to right |
| Parallel (`⊗`) | "Do these together" — space flows top to bottom |
| Interchange | Grid of operations: row-wise = column-wise |
| Monoidal unit (`I`) | "Nothing" — the empty diagram |
| Associativity | Grouping doesn't matter |

---

## The Payoff

Understanding monoidal categories gives us:

1. **Optimization freedom**: Interchange lets us reorganize parallel/sequential structure
2. **Correctness guarantees**: The laws ensure transformations preserve meaning
3. **Compositional reasoning**: Build complex systems from simple, well-understood parts
4. **Visual intuition**: String diagrams make the algebra tangible

**Key insight**: The interchange law is why dataflow programming works — you can think in pipelines or in stages, and the math guarantees consistency.
