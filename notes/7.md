# Session 7: Computation Graphs as Diagrams

*Reading: "Reverse-mode autodiff" overview; Optional: Fong/Spivak/Tuyéras "Backprop as Functor"*

---

## The Core Insight

A computation graph **is** a string diagram. The boxes are operations, the wires are tensors, and evaluation is a categorical fold over the DAG.

```text
        ┌─────┐
   x ───│  W  │───┐
        └─────┘   │   ┌─────┐    ┌──────┐
                  ├───│  +  │────│ ReLU │──── y
        ┌─────┐   │   └─────┘    └──────┘
   1 ───│  b  │───┘
        └─────┘

   y = ReLU(W·x + b)
```

This is exactly what we built in Sessions 4-6: `Diagram<DiffOp>` where `DiffOp` includes `MatMul`, `Add`, `ReLU`, etc.

---

## Evaluation as Categorical Fold

Evaluating a computation graph is a **functor** from diagrams to values:

```text
eval: Diagram → Value
```

This functor must preserve structure:
- Sequential composition: `eval(f ; g) = eval(g)(eval(f)(x))`
- Parallel composition: `eval(f ⊗ g) = (eval(f), eval(g))`

In code, this is a topological traversal:

```rust
impl<O: DiffOp> Diagram<O> {
    /// Evaluate the diagram (forward pass)
    pub fn eval(&self, inputs: &[Tensor]) -> Vec<Tensor> {
        let mut values: HashMap<NodeIndex, Vec<Tensor>> = HashMap::new();

        // Topological order ensures dependencies are computed first
        for node_idx in self.topological_order() {
            let node = &self.graph[node_idx];

            // Gather inputs from predecessor outputs
            let node_inputs: Vec<Tensor> = self.gather_inputs(node_idx, &values);

            // Apply the operation
            let outputs = node.op.forward(&node_inputs);

            values.insert(node_idx, outputs);
        }

        // Return boundary outputs
        self.gather_outputs(&values)
    }
}
```

---

## Const Generic Shapes: Compile-Time Dimension Checking

Session 2 introduced runtime shapes (`Shape { dims: Vec<usize> }`). Now we go further: **const generics** for compile-time guarantees.

### The Problem with Runtime Shapes

```rust
// Runtime: errors discovered when you run the code
let a = Tensor::new(vec![3, 4]);  // 3x4 matrix
let b = Tensor::new(vec![5, 6]);  // 5x6 matrix
let c = a.matmul(&b);             // Runtime error! 4 ≠ 5
```

### The Solution: Const Generic Tensors

```rust
/// A tensor with compile-time known dimensions
pub struct Tensor<T, const ROWS: usize, const COLS: usize> {
    data: [[T; COLS]; ROWS],
}

impl<T, const M: usize, const N: usize> Tensor<T, M, N> {
    /// Matrix multiplication with compile-time dimension checking
    ///
    /// (M × N) · (N × P) → (M × P)
    ///
    /// Note: N must match - this is enforced by the type system!
    pub fn matmul<const P: usize>(
        &self,
        other: &Tensor<T, N, P>,
    ) -> Tensor<T, M, P>
    where
        T: Default + Copy + std::ops::Add<Output = T> + std::ops::Mul<Output = T>,
    {
        let mut result = Tensor::zeros();
        for i in 0..M {
            for j in 0..P {
                for k in 0..N {
                    result.data[i][j] = result.data[i][j]
                        + self.data[i][k] * other.data[k][j];
                }
            }
        }
        result
    }
}
```

### Compile-Time Error Detection

```rust
let a: Tensor<f32, 3, 4> = Tensor::zeros();  // 3x4 matrix
let b: Tensor<f32, 5, 6> = Tensor::zeros();  // 5x6 matrix

// This won't compile!
// let c = a.matmul(&b);
// Error: expected `Tensor<f32, 4, 6>`, found `Tensor<f32, 5, 6>`
//                          ^ must match

let c: Tensor<f32, 4, 6> = Tensor::zeros();  // 4x6 matrix
let d = a.matmul(&c);  // Compiles! Result is Tensor<f32, 3, 6>
```

---

## Const Generic Operations

### Element-wise Operations

```rust
impl<T, const M: usize, const N: usize> Tensor<T, M, N> {
    /// Element-wise addition (same shape required by type system)
    pub fn add(&self, other: &Tensor<T, M, N>) -> Tensor<T, M, N>
    where
        T: Default + Copy + std::ops::Add<Output = T>,
    {
        let mut result = Tensor::zeros();
        for i in 0..M {
            for j in 0..N {
                result.data[i][j] = self.data[i][j] + other.data[i][j];
            }
        }
        result
    }

    /// Element-wise ReLU
    pub fn relu(&self) -> Tensor<T, M, N>
    where
        T: Default + Copy + PartialOrd,
    {
        let mut result = Tensor::zeros();
        let zero = T::default();
        for i in 0..M {
            for j in 0..N {
                result.data[i][j] = if self.data[i][j] > zero {
                    self.data[i][j]
                } else {
                    zero
                };
            }
        }
        result
    }
}
```

### Broadcasting (Compile-Time Shape Inference)

```rust
impl<T, const M: usize, const N: usize> Tensor<T, M, N> {
    /// Add a bias vector (broadcasts across rows)
    ///
    /// (M × N) + (1 × N) → (M × N)
    pub fn add_bias(&self, bias: &Tensor<T, 1, N>) -> Tensor<T, M, N>
    where
        T: Default + Copy + std::ops::Add<Output = T>,
    {
        let mut result = Tensor::zeros();
        for i in 0..M {
            for j in 0..N {
                result.data[i][j] = self.data[i][j] + bias.data[0][j];
            }
        }
        result
    }
}
```

---

## Type-Safe Neural Network Layers

```rust
/// A linear layer with compile-time dimension checking
pub struct Linear<const IN: usize, const OUT: usize> {
    weights: Tensor<f32, OUT, IN>,
    bias: Tensor<f32, 1, OUT>,
}

impl<const IN: usize, const OUT: usize> Linear<IN, OUT> {
    pub fn forward<const BATCH: usize>(
        &self,
        input: &Tensor<f32, BATCH, IN>,
    ) -> Tensor<f32, BATCH, OUT> {
        // input: (BATCH × IN)
        // weights.T: (IN × OUT)
        // result: (BATCH × OUT)
        input
            .matmul(&self.weights.transpose())
            .add_bias(&self.bias)
    }
}

/// A complete network with type-checked layer composition
pub struct MLP<const IN: usize, const HIDDEN: usize, const OUT: usize> {
    layer1: Linear<IN, HIDDEN>,
    layer2: Linear<HIDDEN, OUT>,
}

impl<const IN: usize, const HIDDEN: usize, const OUT: usize> MLP<IN, HIDDEN, OUT> {
    pub fn forward<const BATCH: usize>(
        &self,
        input: &Tensor<f32, BATCH, IN>,
    ) -> Tensor<f32, BATCH, OUT> {
        let h = self.layer1.forward(input).relu();
        self.layer2.forward(&h)
    }
}
```

Usage:

```rust
// Define network architecture at compile time
let mlp: MLP<784, 256, 10> = MLP::new();

// Input shape is checked at compile time
let input: Tensor<f32, 32, 784> = Tensor::zeros();  // batch of 32
let output = mlp.forward(&input);  // Tensor<f32, 32, 10>

// This won't compile - wrong input dimension!
// let bad_input: Tensor<f32, 32, 100> = Tensor::zeros();
// let _ = mlp.forward(&bad_input);  // Error: expected 784, got 100
```

---

## Integrating with Diagrams

The const generic tensors can be used as the "shapes" in our diagram system:

```rust
/// A typed port with const generic dimensions
pub struct TypedPort<const M: usize, const N: usize> {
    _phantom: PhantomData<[[f32; N]; M]>,
}

/// A typed edge ensures dimension compatibility at compile time
pub struct TypedEdge<const M: usize, const N: usize> {
    from: TypedPort<M, N>,
    to: TypedPort<M, N>,  // Must match!
}

/// Composition is type-checked
fn compose<const A: usize, const B: usize, const C: usize>(
    f: impl Fn(Tensor<f32, 1, A>) -> Tensor<f32, 1, B>,
    g: impl Fn(Tensor<f32, 1, B>) -> Tensor<f32, 1, C>,
) -> impl Fn(Tensor<f32, 1, A>) -> Tensor<f32, 1, C> {
    move |x| g(f(x))
}
```

---

## The Categorical View

### Objects = Types

In our const generic system:
- Objects are `Tensor<T, M, N>` types
- Each unique `(M, N)` pair is a different object

### Morphisms = Functions

```text
f: Tensor<f32, M, N> → Tensor<f32, P, Q>
```

### Composition = Function Composition

The type system enforces that `f ; g` is only valid when `cod(f) = dom(g)`:

```rust
// f: (M × N) → (P × Q)
// g: (P × Q) → (R × S)
// f ; g: (M × N) → (R × S)

fn f(x: Tensor<f32, 3, 4>) -> Tensor<f32, 5, 6> { ... }
fn g(x: Tensor<f32, 5, 6>) -> Tensor<f32, 7, 8> { ... }

// Composition is type-safe:
fn fg(x: Tensor<f32, 3, 4>) -> Tensor<f32, 7, 8> {
    g(f(x))
}
```

### Monoidal Product = Tuple

```rust
// f: A → B
// g: C → D
// f ⊗ g: (A, C) → (B, D)

fn tensor<A, B, C, D>(
    f: impl Fn(A) -> B,
    g: impl Fn(C) -> D,
) -> impl Fn((A, C)) -> (B, D) {
    move |(a, c)| (f(a), g(c))
}
```

---

## Why Const Generics Matter

| Aspect | Runtime Shapes | Const Generic Shapes |
|--------|---------------|---------------------|
| Error detection | Runtime | Compile time |
| Performance | Dynamic dispatch possible | Zero-cost, fully inlined |
| Flexibility | Any dimensions | Fixed at compile time |
| Code size | One implementation | Monomorphized per size |
| IDE support | Limited | Full autocomplete |

### When to Use Each

**Const Generics** (this session's focus):
- Known architectures (ResNet-50, BERT, etc.)
- Performance-critical inner loops
- Library APIs where safety matters

**Runtime Shapes** (Session 2):
- Dynamic batch sizes
- User-specified architectures
- Research/experimentation

---

## Summary

| Concept | Implementation |
|---------|---------------|
| Computation graph | `Diagram<DiffOp>` |
| Evaluation | Topological fold (functor) |
| Compile-time shapes | `Tensor<T, M, N>` with const generics |
| Type-safe matmul | `(M×N) · (N×P) → (M×P)` |
| Type-safe layers | `Linear<IN, OUT>` |
| Composition | Rust's type system enforces `cod(f) = dom(g)` |

**Key insight**: The type system is a proof assistant. When `let y = mlp.forward(&x)` compiles, you have a compile-time proof that all dimensions match throughout the network.
