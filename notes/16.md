# Session 16: Full DisCoCat — Sentence Similarity and Applications

## Overview

This session completes the DisCoCat (Distributional Compositional Categorical) implementation by adding:
- Sentence similarity measures
- Verb construction from corpus data
- Entailment and relatedness tasks
- Comparison with baseline models

**Key insight**: DisCoCat unifies distributional semantics (what words mean) with compositional structure (how meanings combine).

## Sentence Similarity

### The Task

Given two sentences, compute how similar their meanings are:
```
"Alice loves Bob" vs "Alice adores Bob"  → high similarity
"Alice loves Bob" vs "Alice hates Bob"   → low similarity
"dog runs" vs "cat walks"                → medium similarity
```

### From Scalars to Vectors

In Session 15.5, sentence meaning was a scalar. For similarity, we need sentence **vectors**:

```
F(S) = ℝᵐ  (instead of ℝ)
```

Now transitive verbs become **order-3 tensors**:
```
F(Nʳ·S·Nˡ) = ℝⁿ ⊗ ℝᵐ ⊗ ℝⁿ
```

### Contraction to Sentence Vector

For "Alice loves Bob":
```
         Alice      loves           Bob
           │      ┌──┴──┐            │
          vᵢ     Tᵢⱼₖ              wₖ
           │     /    \             │
           └────┘      └────────────┘
              ↓              ↓
            Σᵢ            Σₖ
                  ↓
              sⱼ ∈ ℝᵐ  (sentence vector)
```

Formula: `sⱼ = Σᵢₖ vᵢ · Tᵢⱼₖ · wₖ`

### Cosine Similarity

```
sim(s₁, s₂) = (s₁ · s₂) / (‖s₁‖ · ‖s₂‖)
```

Range: [-1, 1], where 1 = identical meaning, 0 = unrelated, -1 = opposite.

## Building Verb Tensors from Data

### The Challenge

Where do verb tensors come from? We can't just make them up!

### Relational Method (Grefenstette & Sadrzadeh)

Build verb tensor from subject-object co-occurrences:
```
loves_ijk = Σ (subj, obj) ∈ corpus  subj_i · basis_j · obj_k
```

Where (subj, obj) are noun vectors that appear with "loves".

### Kronecker Product Method

Simpler: take Kronecker product of typical subject/object:
```
loves = subj_typical ⊗ obj_typical
```

### Copy-Subject Method

For intransitive-like behavior, copy subject:
```
loves_ijk = subj_i · δ_jk
```

This makes "Alice loves Bob" depend mainly on Alice.

## Practical Implementation

### Sentence Vector Computation

```rust
/// Sentence semantics with vector output
pub struct SentenceSemantics {
    noun_dim: usize,
    sentence_dim: usize,
    lexicon: HashMap<String, SentenceTensor>,
}

enum SentenceTensor {
    /// Noun: vector in ℝⁿ
    NounVec(Vec<f64>),
    /// Intransitive verb: matrix ℝⁿ → ℝᵐ
    IntransVerb(Vec<Vec<f64>>),
    /// Transitive verb: tensor ℝⁿ × ℝᵐ × ℝⁿ
    TransVerb(Vec<Vec<Vec<f64>>>),
}

impl SentenceSemantics {
    /// Compute sentence vector
    pub fn sentence_vector(&self, parse: &ParseResult) -> Vec<f64> {
        // ... tensor contraction returning ℝᵐ
    }

    /// Cosine similarity between sentences
    pub fn similarity(&self, s1: &ParseResult, s2: &ParseResult) -> f64 {
        let v1 = self.sentence_vector(s1);
        let v2 = self.sentence_vector(s2);
        cosine_similarity(&v1, &v2)
    }
}
```

### Building Tensors from Word2Vec

```rust
/// Build verb tensor from pre-trained word vectors
pub fn build_verb_tensor(
    verb: &str,
    word2vec: &HashMap<String, Vec<f64>>,
    corpus: &[(String, String)],  // (subject, object) pairs
) -> Vec<Vec<Vec<f64>>> {
    let n = word2vec.values().next().unwrap().len();
    let mut tensor = vec![vec![vec![0.0; n]; n]; n];

    for (subj, obj) in corpus {
        if let (Some(sv), Some(ov)) = (word2vec.get(subj), word2vec.get(obj)) {
            // Outer product contribution
            for i in 0..n {
                for j in 0..n {
                    for k in 0..n {
                        tensor[i][j][k] += sv[i] * ov[k];
                    }
                }
            }
        }
    }

    tensor
}
```

## Evaluation Tasks

### Sentence Similarity (STS Benchmark)

Task: Predict human similarity ratings for sentence pairs.

```
Sentence 1: "A man is playing a guitar"
Sentence 2: "A person plays a musical instrument"
Human rating: 4.2 / 5.0
```

Metric: Spearman/Pearson correlation with human judgments.

### Textual Entailment (SICK Dataset)

Task: Does sentence A entail, contradict, or is neutral to B?

```
A: "A dog is running through the grass"
B: "An animal is moving"
Label: ENTAILMENT
```

DisCoCat approach: If sim(A, B) > threshold and A is more specific, then entailment.

### Word Sense Disambiguation

Context determines which word vector to use:
```
"bank" near "river" → geographical sense
"bank" near "money" → financial sense
```

Contextual composition handles this naturally.

## Comparison with Baselines

### Bag-of-Words

Ignores word order:
```
sim("dog bites man", "man bites dog") = 1.0  (same words!)
```

DisCoCat captures difference through tensor contraction.

### Addition Model

```
sentence = Σ word_vectors
```

Better than BoW but still ignores structure.

### Multiplication Model

```
sentence = Π word_vectors  (element-wise)
```

Captures some interaction but not grammatical roles.

### DisCoCat Advantage

- Grammatically-aware composition
- Subject/object roles distinguished
- Verb mediates relationship
- Compositional (functor laws)

## Frobenius Algebras (Advanced)

For more complex sentences, we need **Frobenius algebras** on noun space:

### The Problem

How do we handle:
- Relative clauses: "the dog that runs"
- Coordination: "Alice and Bob"
- Questions: "Who runs?"

### Frobenius Structure

A Frobenius algebra on V has:
- Multiplication: V ⊗ V → V (merge)
- Unit: I → V
- Comultiplication: V → V ⊗ V (copy)
- Counit: V → I

These satisfy:
```
        ╭──╮        ╭──╮
    ────┤  ├────  = │  │ = ────┬────
        ╰──╯        ╰──╯       │
                               │
```

### Application: Relative Clauses

"dog that runs" = dog · (runs composed via Frobenius)

The Frobenius structure lets us "copy" the noun into the relative clause.

## Density Matrices (Advanced)

For handling ambiguity and uncertainty:

### Words as Density Matrices

Instead of vectors, use density matrices ρ ∈ ℝⁿˣⁿ:
```
"bank" → ρ_bank = p₁|river⟩⟨river| + p₂|money⟩⟨money|
```

This represents a mixture of senses.

### Composition via CPM

Completely Positive Maps preserve density matrix structure:
```
F(verb): ρ_subj ⊗ ρ_obj → ρ_sentence
```

## Implementation Sketch

```rust
/// Full DisCoCat system
pub struct DisCoCat {
    grammar: Grammar,
    semantics: SentenceSemantics,
}

impl DisCoCat {
    /// Parse and compute meaning
    pub fn meaning(&self, sentence: &[&str]) -> Result<Vec<f64>, NlpError> {
        let parse = self.grammar.parse(sentence)?;
        if !parse.is_grammatical() {
            return Err(NlpError::Ungrammatical);
        }
        Ok(self.semantics.sentence_vector(&parse))
    }

    /// Sentence similarity
    pub fn similarity(&self, s1: &[&str], s2: &[&str]) -> Result<f64, NlpError> {
        let v1 = self.meaning(s1)?;
        let v2 = self.meaning(s2)?;
        Ok(cosine_similarity(&v1, &v2))
    }

    /// Batch similarity for evaluation
    pub fn evaluate_sts(&self, pairs: &[(Vec<&str>, Vec<&str>, f64)]) -> f64 {
        let predictions: Vec<f64> = pairs.iter()
            .map(|(s1, s2, _)| self.similarity(s1, s2).unwrap_or(0.0))
            .collect();
        let gold: Vec<f64> = pairs.iter().map(|(_, _, g)| *g).collect();
        spearman_correlation(&predictions, &gold)
    }
}
```

## Exercises

1. **Similarity task**: Implement sentence vectors and compute similarity between:
   - "Alice loves Bob" vs "Alice adores Bob"
   - "dog runs" vs "cat walks"
   - "man bites dog" vs "dog bites man"

2. **Verb tensor**: Build a tensor for "chases" from the pairs:
   - (dog, cat), (cat, mouse), (wolf, deer)
   Using the relational method.

3. **Baseline comparison**: For the same sentence pairs, compute:
   - Bag-of-words similarity
   - Addition model similarity
   - DisCoCat similarity
   Compare results.

4. **Frobenius**: For "dog that runs", show how the Frobenius comultiplication copies the dog vector.

## Summary

| Component | Description |
|-----------|-------------|
| Sentence vector | F(S) = ℝᵐ |
| Transitive verb | Order-3 tensor ∈ ℝⁿ ⊗ ℝᵐ ⊗ ℝⁿ |
| Similarity | Cosine of sentence vectors |
| Verb construction | Relational, Kronecker, or copy methods |
| Evaluation | STS correlation, entailment accuracy |
| Extensions | Frobenius algebras, density matrices |

## Connections

- **Session 3**: Functors ensure compositional coherence
- **Session 15**: Pregroup grammar provides structure
- **Session 15.5**: Tensor semantics gives meaning
- **Session 16**: Full system with similarity and evaluation

## Reading

- Kartsaklis, Sadrzadeh, Pulman: "A Unified Sentence Space for Categorical Distributional-Compositional Semantics"
- Coecke, Sadrzadeh, Clark: "Mathematical Foundations for a Compositional Distributional Model of Meaning"
- Piedeleu et al.: "Open System Categorical Quantum Semantics in Natural Language Processing"
- Bankova et al.: "Graded Hyponymy for Compositional Distributional Semantics"

## Next

Session 17: Monoidal categories and string diagrams — the general framework underlying all our constructions.
