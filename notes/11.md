# Session 11: Stochastic Maps (Finite Markov Kernels)

*Reading: Tobias Fritz "Markov categories intro notes"; Baez/Fong "stochastic matrices as morphisms"*

---

## The Core Insight

Probability is **compositional**. A stochastic process from X to Y is a morphism `X → Dist(Y)`, and these compose just like functions—but with probability distributions flowing through.

```text
Deterministic:    X ──f──▶ Y ──g──▶ Z
                  x ↦ f(x) ↦ g(f(x))

Stochastic:       X ──K──▶ Dist(Y) ──L──▶ Dist(Z)
                  x ↦ K(·|x) ↦ Σᵧ K(y|x) · L(·|y)
```

In finite-dimensional probability, these morphisms are **row-stochastic matrices**.

---

## Stochastic Maps

### Definition

A **stochastic map** (Markov kernel) from X to Y is a function:

```text
K: X → Dist(Y)
```

For each input x ∈ X, we get a probability distribution over Y.

### Finite Case: Row-Stochastic Matrices

When X = {1, ..., n} and Y = {1, ..., m}, a stochastic map is an n × m matrix where:

```text
K[i,j] = P(output = j | input = i)
```

Each row sums to 1:

```text
Σⱼ K[i,j] = 1  for all i
```

```text
Example: Fair coin flip from {heads, tails} to {0, 1}

K = [ 0.5  0.5 ]  ← P(0|heads), P(1|heads)
    [ 0.5  0.5 ]  ← P(0|tails), P(1|tails)
```

---

## Probability Distributions

A **probability distribution** over a finite set {1, ..., n} is a vector p where:

```text
p[i] ≥ 0  for all i
Σᵢ p[i] = 1
```

```rust
/// A probability distribution over a finite set.
pub struct Dist {
    /// Probability vector (sums to 1)
    pub p: Vec<f32>,
}

const PROB_TOLERANCE: f32 = 1e-6;

impl Dist {
    pub fn new(p: Vec<f32>) -> Result<Self, ProbError> {
        let sum: f32 = p.iter().sum();
        if (sum - 1.0).abs() > PROB_TOLERANCE {
            return Err(ProbError::NotNormalized { sum });
        }
        if p.iter().any(|&x| x < -PROB_TOLERANCE) {
            return Err(ProbError::NegativeProbability);
        }
        Ok(Self { p })
    }

    /// Create from unnormalized weights
    pub fn from_weights(weights: Vec<f32>) -> Result<Self, ProbError> {
        let sum: f32 = weights.iter().sum();
        if sum <= 0.0 {
            return Err(ProbError::ZeroWeights);
        }
        let p: Vec<f32> = weights.iter().map(|w| w / sum).collect();
        Self::new(p)
    }

    /// Uniform distribution over n elements
    pub fn uniform(n: usize) -> Self {
        Self { p: vec![1.0 / n as f32; n] }
    }

    /// Point mass at index i
    pub fn point(n: usize, i: usize) -> Self {
        let mut p = vec![0.0; n];
        p[i] = 1.0;
        Self { p }
    }
}
```

---

## Markov Kernels

A **Markov kernel** (stochastic matrix) from n states to m states:

```rust
/// A Markov kernel (stochastic map) between finite sets.
pub struct Kernel {
    /// Row-stochastic matrix: k[i][j] = P(output=j | input=i)
    /// Shape: (n_inputs, n_outputs)
    pub k: Vec<Vec<f32>>,
    pub n_inputs: usize,
    pub n_outputs: usize,
}

impl Kernel {
    pub fn new(k: Vec<Vec<f32>>) -> Result<Self, ProbError> {
        if k.is_empty() {
            return Err(ProbError::EmptyKernel);
        }

        let n_inputs = k.len();
        let n_outputs = k[0].len();

        // Verify each row sums to 1
        for (i, row) in k.iter().enumerate() {
            if row.len() != n_outputs {
                return Err(ProbError::RaggedMatrix);
            }
            let sum: f32 = row.iter().sum();
            if (sum - 1.0).abs() > PROB_TOLERANCE {
                return Err(ProbError::RowNotNormalized { row: i, sum });
            }
        }

        Ok(Self { k, n_inputs, n_outputs })
    }

    /// Identity kernel: deterministically map i to i
    pub fn identity(n: usize) -> Self {
        let k: Vec<Vec<f32>> = (0..n)
            .map(|i| {
                let mut row = vec![0.0; n];
                row[i] = 1.0;
                row
            })
            .collect();
        Self { k, n_inputs: n, n_outputs: n }
    }

    /// Constant kernel: always output the same distribution
    pub fn constant(n_inputs: usize, dist: &Dist) -> Self {
        let k: Vec<Vec<f32>> = (0..n_inputs)
            .map(|_| dist.p.clone())
            .collect();
        Self {
            k,
            n_inputs,
            n_outputs: dist.p.len(),
        }
    }
}
```

---

## Composition of Kernels

This is where the magic happens. Composing two kernels:

```text
K: X → Dist(Y)   (n × m matrix)
L: Y → Dist(Z)   (m × p matrix)

(K ; L): X → Dist(Z)   (n × p matrix)

(K ; L)[i,k] = Σⱼ K[i,j] · L[j,k]
```

This is just **matrix multiplication**! But the categorical view is:

```text
For each input x:
  1. Sample y from K(·|x)
  2. Sample z from L(·|y)
  3. The resulting distribution over z is (K;L)(·|x)
```

```rust
impl Kernel {
    /// Compose two kernels: self ; other
    /// self: n → m, other: m → p
    /// result: n → p
    pub fn compose(&self, other: &Kernel) -> Result<Kernel, ProbError> {
        if self.n_outputs != other.n_inputs {
            return Err(ProbError::ShapeMismatch {
                expected: self.n_outputs,
                got: other.n_inputs,
            });
        }

        let n = self.n_inputs;
        let m = self.n_outputs;
        let p = other.n_outputs;

        let mut result = vec![vec![0.0; p]; n];

        for i in 0..n {
            for k in 0..p {
                let mut sum = 0.0;
                for j in 0..m {
                    sum += self.k[i][j] * other.k[j][k];
                }
                result[i][k] = sum;
            }
        }

        Kernel::new(result)
    }
}
```

---

## Applying Kernels to Distributions

A kernel transforms a distribution on the input to a distribution on the output:

```text
p: Dist(X)
K: X → Dist(Y)

K(p): Dist(Y)

K(p)[j] = Σᵢ p[i] · K[i,j]
```

This is vector-matrix multiplication:

```rust
impl Kernel {
    /// Apply kernel to a distribution
    pub fn apply(&self, dist: &Dist) -> Result<Dist, ProbError> {
        if dist.p.len() != self.n_inputs {
            return Err(ProbError::ShapeMismatch {
                expected: self.n_inputs,
                got: dist.p.len(),
            });
        }

        let mut result = vec![0.0; self.n_outputs];
        for j in 0..self.n_outputs {
            for i in 0..self.n_inputs {
                result[j] += dist.p[i] * self.k[i][j];
            }
        }

        Dist::new(result)
    }
}
```

---

## The Category of Stochastic Maps

### Objects
Finite sets (or just natural numbers representing cardinality)

### Morphisms
Markov kernels (row-stochastic matrices)

### Composition
Matrix multiplication (which preserves row-stochastic property!)

### Identity
The identity matrix (deterministic identity function)

```rust
/// The category of finite stochastic maps
pub struct FinStoch;

impl Category for FinStoch {
    type Object = usize;  // Cardinality of finite set
    type Morphism = Kernel;

    fn id(n: usize) -> Kernel {
        Kernel::identity(n)
    }

    fn compose(f: &Kernel, g: &Kernel) -> Result<Kernel, Error> {
        f.compose(g)
    }
}
```

---

## Why Row-Stochastic?

The convention matters:

- **Row-stochastic**: Each row sums to 1. Entry K[i,j] = P(output=j | input=i)
- **Column-stochastic**: Each column sums to 1. Entry K[i,j] = P(input=i | output=j)

We use row-stochastic because:
1. Composition is left-to-right (like function composition in Rust)
2. Applying to a row vector: `result = input @ K`
3. Matches the categorical convention: morphisms go from domain to codomain

---

## Examples

### Fair Die

```rust
// A fair 6-sided die (1 input, 6 outputs)
let die = Kernel::constant(1, &Dist::uniform(6));
```

### Noisy Channel

```rust
// Binary symmetric channel with error rate 0.1
// Input: {0, 1}, Output: {0, 1}
let channel = Kernel::new(vec![
    vec![0.9, 0.1],  // P(output | input=0)
    vec![0.1, 0.9],  // P(output | input=1)
]).unwrap();
```

### Weather Transition

```rust
// Weather: {Sunny=0, Rainy=1}
// Tomorrow's weather depends on today's
let weather = Kernel::new(vec![
    vec![0.8, 0.2],  // Sunny → 80% Sunny, 20% Rainy
    vec![0.4, 0.6],  // Rainy → 40% Sunny, 60% Rainy
]).unwrap();

// After 2 days:
let two_days = weather.compose(&weather).unwrap();
```

---

## Verification

Row sums must equal 1 (within tolerance):

```rust
#[test]
fn test_kernel_normalization() {
    // Valid kernel
    let k = Kernel::new(vec![
        vec![0.3, 0.7],
        vec![0.5, 0.5],
    ]);
    assert!(k.is_ok());

    // Invalid: row sums to 0.9
    let bad = Kernel::new(vec![
        vec![0.3, 0.6],  // sum = 0.9
        vec![0.5, 0.5],
    ]);
    assert!(bad.is_err());
}

#[test]
fn test_composition_preserves_stochastic() {
    let k1 = Kernel::new(vec![
        vec![0.5, 0.5],
        vec![0.3, 0.7],
    ]).unwrap();

    let k2 = Kernel::new(vec![
        vec![0.6, 0.4],
        vec![0.2, 0.8],
    ]).unwrap();

    let composed = k1.compose(&k2).unwrap();

    // Each row should still sum to 1
    for row in &composed.k {
        let sum: f32 = row.iter().sum();
        assert!((sum - 1.0).abs() < 1e-6);
    }
}
```

---

## Error Types

```rust
#[derive(Debug, Clone)]
pub enum ProbError {
    /// Distribution doesn't sum to 1
    NotNormalized { sum: f32 },

    /// Negative probability
    NegativeProbability,

    /// All weights are zero (can't normalize)
    ZeroWeights,

    /// Empty kernel
    EmptyKernel,

    /// Rows have different lengths
    RaggedMatrix,

    /// A row doesn't sum to 1
    RowNotNormalized { row: usize, sum: f32 },

    /// Shape mismatch for composition
    ShapeMismatch { expected: usize, got: usize },
}
```

---

## Connection to Neural Networks

Softmax outputs are probability distributions:

```text
Neural net:  ℝⁿ → ℝᵐ → softmax → Dist({1,...,m})
```

The softmax function turns logits into a valid probability distribution:

```rust
pub fn softmax(logits: &[f32]) -> Dist {
    let max = logits.iter().cloned().fold(f32::NEG_INFINITY, f32::max);
    let exp: Vec<f32> = logits.iter().map(|x| (x - max).exp()).collect();
    let sum: f32 = exp.iter().sum();
    Dist { p: exp.iter().map(|x| x / sum).collect() }
}
```

---

## Summary

| Concept | Implementation |
|---------|----------------|
| Probability distribution | Vector summing to 1 |
| Stochastic map | Row-stochastic matrix |
| Composition | Matrix multiplication |
| Identity | Identity matrix |
| Apply to distribution | Vector-matrix product |
| Category | FinStoch (finite stochastic maps) |

**Key insight**: Probability theory has a compositional structure. Stochastic maps form a category where:
- Objects are (finite) sample spaces
- Morphisms are conditional distributions
- Composition is marginalization over intermediate variables

This is the foundation for Bayesian networks, hidden Markov models, and probabilistic programming.

---

## Next: Session 12

With stochastic maps in place, Session 12 builds:
- **Bayesian networks** as composed kernels
- **Marginalization** as summing out variables
- The connection between graphical models and string diagrams
