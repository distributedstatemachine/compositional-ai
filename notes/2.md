# Session 2: Types/Shapes as Objects

## Overview

In this session, we treat **tensor shapes as categorical objects**. The key insight is that shapes must "compose correctly" — the output shape of one layer must match the input shape of the next. This is exactly the categorical requirement that morphisms compose when their domains/codomains match.

We also introduce **errors as first-class citizens** — representing "non-composable morphisms" as explicit error types.

## Key Concepts

### Shapes as Types

In ML, tensors have shapes:
- Scalar: `[]` or `()`
- Vector: `[n]` or `(n,)`
- Matrix: `[m, n]`
- Batch of images: `[batch, channels, height, width]`

**Categorical view**: Shapes are objects. A layer transforming shape A to shape B is a morphism A → B.

### Shape Composition Rule

Two morphisms f: A → B and g: B → C compose if and only if:
```
output_shape(f) = input_shape(g)
```

**Example:**
```
Linear(784 → 256): [batch, 784] → [batch, 256]
Linear(256 → 10):  [batch, 256] → [batch, 10]

Composition works! Input 784, hidden 256, output 10.
```

**Non-example:**
```
Linear(784 → 256): [batch, 784] → [batch, 256]
Linear(128 → 10):  [batch, 128] → [batch, 10]  ← MISMATCH!

Cannot compose: 256 ≠ 128
```

### Type-Level vs Runtime Shape Checking

There's a fundamental tradeoff:

| Approach | Pros | Cons |
|----------|------|------|
| Runtime (`Vec<usize>`) | Flexible, works with dynamic graphs | Errors at runtime |
| Compile-time (const generics) | Errors at compile time | Less flexible |

**Our strategy**: Use runtime shapes for flexibility, but provide a `StaticShape` trait for compile-time guarantees when needed.

## Implementation

### Shape Type

```rust
/// Type identifier for categorical objects
#[derive(Debug, Clone, PartialEq, Eq, Hash)]
pub struct TypeId(pub &'static str);

/// A shape represents the "type" of a tensor
#[derive(Debug, Clone, PartialEq, Eq)]
pub struct Shape {
    pub ty: TypeId,
    pub dims: Vec<usize>,
}

impl Shape {
    /// Scalar shape (0-dimensional)
    pub fn scalar() -> Self {
        Shape {
            ty: TypeId("scalar"),
            dims: vec![],
        }
    }

    /// Vector shape (1-dimensional)
    pub fn vector(n: usize) -> Self {
        Shape {
            ty: TypeId("vector"),
            dims: vec![n],
        }
    }

    /// Matrix shape (2-dimensional)
    pub fn matrix(rows: usize, cols: usize) -> Self {
        Shape {
            ty: TypeId("matrix"),
            dims: vec![rows, cols],
        }
    }

    /// Check if this shape can be the input to a morphism expecting `other`
    pub fn is_compatible_with(&self, other: &Shape) -> bool {
        self.dims == other.dims
    }
}
```

### Errors as First-Class

Non-composable morphisms should produce clear errors:

```rust
use thiserror::Error;

#[derive(Debug, Clone, Error)]
pub enum CoreError {
    #[error("Shape mismatch: expected {expected:?}, got {got:?}")]
    ShapeMismatch { expected: Shape, got: Shape },

    #[error("Dimension mismatch at position {position}: expected {expected}, got {got}")]
    DimensionMismatch {
        position: usize,
        expected: usize,
        got: usize,
    },

    #[error("Cannot compose: output {output:?} incompatible with input {input:?}")]
    CompositionError { output: Shape, input: Shape },
}
```

### Static Shapes (Compile-Time Checking)

For cases where we want compile-time guarantees:

```rust
/// Marker trait for compile-time shape checking
pub trait StaticShape {
    const DIMS: &'static [usize];

    fn to_shape() -> Shape;
}

/// Example: 3x3 matrix known at compile time
pub struct Mat3x3;

impl StaticShape for Mat3x3 {
    const DIMS: &'static [usize] = &[3, 3];

    fn to_shape() -> Shape {
        Shape::matrix(3, 3)
    }
}

/// Example: 784-dimensional vector (MNIST input)
pub struct Vec784;

impl StaticShape for Vec784 {
    const DIMS: &'static [usize] = &[784];

    fn to_shape() -> Shape {
        Shape::vector(784)
    }
}
```

With const generics (Rust 1.51+):

```rust
/// Generic static shape using const generics
pub struct StaticVec<const N: usize>;

impl<const N: usize> StaticShape for StaticVec<N> {
    const DIMS: &'static [usize] = &[N];

    fn to_shape() -> Shape {
        Shape::vector(N)
    }
}

pub struct StaticMat<const M: usize, const N: usize>;

impl<const M: usize, const N: usize> StaticShape for StaticMat<M, N> {
    const DIMS: &'static [usize] = &[M, N];

    fn to_shape() -> Shape {
        Shape::matrix(M, N)
    }
}
```

## Shape Broadcasting

Many operations support **broadcasting** — automatically expanding shapes:

```
[1, 3] + [4, 3] → [4, 3]  (broadcast first dim)
[3]    + [4, 3] → [4, 3]  (prepend dim, then broadcast)
```

Broadcasting rules:
1. Align shapes from the right
2. Dimensions must be equal OR one must be 1
3. Broadcast dimension 1 to match the other

```rust
impl Shape {
    /// Check if two shapes can be broadcast together
    pub fn can_broadcast(&self, other: &Shape) -> bool {
        let (longer, shorter) = if self.dims.len() >= other.dims.len() {
            (&self.dims, &other.dims)
        } else {
            (&other.dims, &self.dims)
        };

        let offset = longer.len() - shorter.len();
        for (i, &dim) in shorter.iter().enumerate() {
            let other_dim = longer[offset + i];
            if dim != other_dim && dim != 1 && other_dim != 1 {
                return false;
            }
        }
        true
    }

    /// Compute the broadcast result shape
    pub fn broadcast_with(&self, other: &Shape) -> Result<Shape, CoreError> {
        if !self.can_broadcast(other) {
            return Err(CoreError::ShapeMismatch {
                expected: self.clone(),
                got: other.clone(),
            });
        }

        let max_len = self.dims.len().max(other.dims.len());
        let mut result = vec![0; max_len];

        for i in 0..max_len {
            let d1 = self.dims.get(self.dims.len().wrapping_sub(1 + i)).unwrap_or(&1);
            let d2 = other.dims.get(other.dims.len().wrapping_sub(1 + i)).unwrap_or(&1);
            result[max_len - 1 - i] = (*d1).max(*d2);
        }

        Ok(Shape {
            ty: TypeId("broadcast"),
            dims: result,
        })
    }
}
```

## Matrix Multiplication Shapes

Matrix multiplication has specific shape requirements:

```
[m, k] × [k, n] → [m, n]
```

The inner dimensions must match (k = k).

```rust
impl Shape {
    /// Check if this shape can be matrix-multiplied with other
    pub fn can_matmul(&self, other: &Shape) -> bool {
        if self.dims.len() < 2 || other.dims.len() < 2 {
            return false;
        }
        // Inner dimensions must match
        self.dims[self.dims.len() - 1] == other.dims[other.dims.len() - 2]
    }

    /// Compute the matmul result shape
    pub fn matmul_with(&self, other: &Shape) -> Result<Shape, CoreError> {
        if !self.can_matmul(other) {
            return Err(CoreError::CompositionError {
                output: self.clone(),
                input: other.clone(),
            });
        }

        let m = self.dims[self.dims.len() - 2];
        let n = other.dims[other.dims.len() - 1];

        Ok(Shape::matrix(m, n))
    }
}
```

## Connection to Categories

| Categorical Concept | Shape Interpretation |
|--------------------|---------------------|
| Object | Shape (e.g., `[batch, 784]`) |
| Morphism | Layer/operation with input/output shapes |
| Composition | Sequential layers (shapes must match) |
| Identity | Identity layer (preserves shape) |
| Composability | Output shape = next input shape |

## Exercises

1. **Shape compatibility**: Given shapes `[32, 784]` and `[784, 256]`, can they be composed via matmul? What's the result?

2. **Broadcasting**: What is `[3, 1, 5] broadcast with [4, 5]`? Draw the alignment.

3. **Static shapes**: Define a `StaticShape` for a `[batch, 3, 224, 224]` tensor (batch of RGB images).

4. **Error types**: Design a `DimensionError` for when specific dimension indices don't match.

## Summary

| Concept | Implementation |
|---------|---------------|
| Shape | `struct Shape { ty: TypeId, dims: Vec<usize> }` |
| Compatibility | `output_shape == input_shape` |
| Error | `CoreError::ShapeMismatch { expected, got }` |
| Static shape | `trait StaticShape { const DIMS: &'static [usize]; }` |
| Broadcasting | Dimension 1 expands to match |

## Reading

- Fong & Spivak, *Seven Sketches*: Types as interfaces
- Rust `typenum` crate docs (for type-level numerics)
- NumPy broadcasting rules documentation

## Next

Session 3: Functors + Naturality — structure-preserving maps between categories.
