# Session 14.5 (Optional): Continuous Distributions and Gaussian Kernels

## Overview

Not all probability is finite — continuous distributions are essential for real ML. This session extends our categorical treatment of stochastic maps to the continuous case, focusing on Gaussian kernels.

## Key Concepts

### Why Continuous?

Discrete distributions (FinStoch) are great for:
- Classification outputs
- Discrete state spaces
- Exact computation

But ML needs continuous distributions for:
- Regression outputs
- Latent spaces (VAEs)
- Uncertainty quantification
- Real-valued parameters

### Gaussian Kernels

A **Gaussian kernel** is a stochastic map `K: X → Y` where the output is normally distributed:

```
K: x ↦ N(μ(x), σ²(x))
```

The simplest case: **linear Gaussian kernel**
```
K: x ↦ N(ax + b, σ²)
```

This is a morphism in the category of Gaussian distributions!

### The Category Gauss

**Objects:** Euclidean spaces ℝⁿ

**Morphisms:** Gaussian channels — maps that send points to Gaussian distributions
- `K: ℝⁿ → ℝᵐ` is a morphism if `K(x) = N(Ax + b, Σ)` for some:
  - Linear map `A: ℝⁿ → ℝᵐ`
  - Bias vector `b ∈ ℝᵐ`
  - Covariance matrix `Σ ∈ ℝᵐˣᵐ` (positive semidefinite)

**Identity:** `id: x ↦ N(x, 0)` = deterministic identity (zero variance)

**Composition:** This is the key insight!

## Composition of Gaussian Kernels = Convolution

If we have:
- `K₁: X → Y` with `K₁(x) = N(A₁x + b₁, Σ₁)`
- `K₂: Y → Z` with `K₂(y) = N(A₂y + b₂, Σ₂)`

Then the composition `K₂ ∘ K₁: X → Z` is:

```
(K₂ ∘ K₁)(x) = N(A₂A₁x + A₂b₁ + b₂, A₂Σ₁A₂ᵀ + Σ₂)
```

**In words:**
- Means compose linearly: `μ₂ ∘ μ₁`
- Variances add (after transformation): `A₂Σ₁A₂ᵀ + Σ₂`

### Simple 1D Example

If:
- `K₁: x ↦ N(ax, σ₁²)` (scale by `a`, add noise `σ₁²`)
- `K₂: y ↦ N(by, σ₂²)` (scale by `b`, add noise `σ₂²`)

Then:
```
K₂ ∘ K₁: x ↦ N(abx, b²σ₁² + σ₂²)
```

The variance formula `b²σ₁² + σ₂²` shows:
- Input noise gets scaled by `b²`
- Then we add output noise `σ₂²`
- This is **convolution** of Gaussians!

## Why This Matters for ML

### 1. Variational Autoencoders (VAEs)

The encoder outputs Gaussian parameters:
```
Encoder: x ↦ (μ(x), σ²(x))
```

The decoder samples from a Gaussian:
```
Decoder: z ↦ N(f(z), σ²_decode)
```

The full model is a composition of Gaussian kernels!

### 2. Bayesian Linear Regression

Prior: `w ~ N(0, σ²_prior I)`
Likelihood: `y | x, w ~ N(w·x, σ²_noise)`

The posterior is also Gaussian — closed under conditioning.

### 3. Gaussian Processes

A GP is an infinite-dimensional Gaussian distribution:
```
f ~ GP(m(x), k(x, x'))
```

Conditioning on data gives another GP — categorical structure!

### 4. Kalman Filters

State transition: `x_{t+1} | x_t ~ N(Ax_t, Q)`
Observation: `y_t | x_t ~ N(Cx_t, R)`

The Kalman filter is literally composition in Gauss!

## The Reparameterization Trick

For backprop through Gaussian sampling:

**Problem:** Can't differentiate through sampling `z ~ N(μ, σ²)`

**Solution:** Reparameterize as deterministic function of noise:
```
ε ~ N(0, 1)           (sample standard normal)
z = μ + σ · ε         (deterministic transformation)
```

**Categorical view:** Factor the kernel as:
```
K: x ↦ N(μ(x), σ²(x))
```
becomes
```
K = (noise source) ; (deterministic transform)
```

This is expressing a stochastic map as composition with a "pure noise" morphism!

## Connections to Information Geometry

The space of Gaussians forms a **Riemannian manifold** with the Fisher information metric:

```
ds² = dμ²/σ² + 2dσ²/σ²
```

Natural gradient descent in VAEs exploits this geometry.

## Summary

| Concept | FinStoch (Discrete) | Gauss (Continuous) |
|---------|--------------------|--------------------|
| Objects | Finite sets | Euclidean spaces ℝⁿ |
| Morphisms | Stochastic matrices | Gaussian channels |
| Composition | Matrix multiplication | Convolution formula |
| Identity | Identity matrix | Delta distribution |
| Key operation | Marginalization | Integration |

The same categorical structure applies — just with continuous morphisms!

## Exercises

1. **Composition calculation:** If `K₁: x ↦ N(2x, 1)` and `K₂: y ↦ N(3y, 4)`, compute `K₂ ∘ K₁`.

2. **VAE as diagram:** Draw the VAE encoder-decoder as a string diagram with Gaussian kernels.

3. **Kalman composition:** Show that two Kalman filter steps compose correctly using the Gaussian kernel composition formula.

4. **Entropy of composition:** If `H(K)` is the differential entropy added by a Gaussian kernel, how does `H(K₂ ∘ K₁)` relate to `H(K₁)` and `H(K₂)`?

## Reading

- Fritz, "A synthetic approach to Markov kernels" — Section on continuous kernels
- Kingma & Welling, "Auto-Encoding Variational Bayes" — The reparameterization trick
- Bishop, "Pattern Recognition and Machine Learning" — Chapter 2 on Gaussians

## Next

Session 15: Counterfactuals — "What would have happened if...?"
