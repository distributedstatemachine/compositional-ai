# Session 9: Reverse-Mode Autodiff (VJP Rules)

*Reading: Backprop chain rule derivation; Fong/Spivak/Tuyéras "Backprop as Functor"*

---

## The Core Insight

The backward pass is a **functor to the opposite category**. This isn't a metaphor—it's the literal mathematical structure of backpropagation.

```text
Forward:  A ──f──▶ B ──g──▶ C

Backward: A ◀──vjp(f)── B ◀──vjp(g)── C
```

If forward composition is `f ; g`, then backward is `vjp(g) ; vjp(f)`—the same operations in **reverse order**.

---

## What is a VJP?

**VJP = Vector-Jacobian Product**

For a function `f: ℝⁿ → ℝᵐ`, the Jacobian is an `m × n` matrix of partial derivatives:

```text
        ∂f₁/∂x₁  ∂f₁/∂x₂  ...  ∂f₁/∂xₙ
J_f =   ∂f₂/∂x₁  ∂f₂/∂x₂  ...  ∂f₂/∂xₙ
        ...
        ∂fₘ/∂x₁  ∂fₘ/∂x₂  ...  ∂fₘ/∂xₙ
```

The VJP computes `vᵀ · J_f` for a vector `v` (the "upstream gradient"):

```text
vjp(f, x, v) = vᵀ · J_f(x)
```

This gives us the gradient with respect to the inputs, weighted by how much we care about each output.

---

## Why VJP Instead of the Full Jacobian?

Computing the full Jacobian is expensive: O(m × n) operations.

VJP is cheap: O(n) operations, because we only need the product with one vector.

For neural networks with scalar loss (m = 1), we start with `v = 1` and propagate backward. Each VJP gives us exactly the gradients we need.

---

## The Chain Rule as Functor Composition

The chain rule says:

```text
∂L/∂x = ∂L/∂y · ∂y/∂x
```

In VJP form, if `y = f(x)` and we have upstream gradient `∂L/∂y`:

```text
∂L/∂x = vjp(f, x, ∂L/∂y)
```

For a composition `f ; g`:

```text
x ──f──▶ y ──g──▶ z

∂L/∂y = vjp(g, y, ∂L/∂z)    // backward through g first
∂L/∂x = vjp(f, x, ∂L/∂y)    // then backward through f
```

This is `vjp(g) ; vjp(f)`—the **opposite order** of the forward pass!

---

## VJP Rules for Common Operations

### Add: `z = x + y`

```text
∂z/∂x = 1,  ∂z/∂y = 1

vjp(Add, (x, y), grad_z):
    grad_x = grad_z
    grad_y = grad_z
```

The gradient flows equally to both inputs.

### Mul (element-wise): `z = x * y`

```text
∂z/∂x = y,  ∂z/∂y = x

vjp(Mul, (x, y), grad_z):
    grad_x = grad_z * y
    grad_y = grad_z * x
```

Each input's gradient is scaled by the other input.

### ReLU: `y = max(0, x)`

```text
∂y/∂x = 1 if x > 0, else 0

vjp(ReLU, x, grad_y):
    grad_x = grad_y * (x > 0 ? 1 : 0)
```

Gradient passes through where input was positive, blocked where negative.

### MatMul: `C = A @ B`

```text
∂L/∂A = (∂L/∂C) @ Bᵀ
∂L/∂B = Aᵀ @ (∂L/∂C)

vjp(MatMul, (A, B), grad_C):
    grad_A = grad_C @ Bᵀ
    grad_B = Aᵀ @ grad_C
```

This is where linear algebra meets autodiff.

### SumAll: `s = sum(x)`

```text
∂s/∂xᵢ = 1 for all i

vjp(SumAll, x, grad_s):
    grad_x = broadcast(grad_s, shape(x))
```

Scalar gradient broadcasts back to input shape.

### Copy: `(y₁, y₂, ...) = (x, x, ...)`

```text
vjp(Copy, x, (grad_y₁, grad_y₂, ...)):
    grad_x = grad_y₁ + grad_y₂ + ...
```

Gradients from all copies sum together (adjoint of diagonal is trace).

---

## The Backward Algorithm

```rust
fn backward(
    graph: &DiffGraph,
    forward_values: &HashMap<NodeIndex, Vec<RTensor>>,
    seed_grad: RTensor,
) -> HashMap<NodeIndex, Vec<RTensor>> {
    let mut grads: HashMap<NodeIndex, Vec<RTensor>> = HashMap::new();

    // Initialize output gradient with seed
    let output_node = graph.output_nodes[0];
    grads.insert(output_node, vec![seed_grad]);

    // Process in REVERSE topological order
    for node_idx in graph.topological_order().into_iter().rev() {
        let node = &graph.diagram.graph[node_idx];
        let output_grad = &grads[&node_idx];

        // Get forward values for this node
        let inputs = gather_inputs(node_idx, forward_values);

        // Compute input gradients via VJP
        let input_grads = node.op.vjp(&inputs, output_grad);

        // Accumulate gradients to predecessor nodes
        for (pred_idx, grad) in predecessors(node_idx).zip(input_grads) {
            grads.entry(pred_idx)
                .or_insert_with(Vec::new)
                .push(grad);
        }
    }

    // Sum accumulated gradients (for nodes with multiple outputs)
    for (_, grads_list) in grads.iter_mut() {
        if grads_list.len() > 1 {
            *grads_list = vec![sum_tensors(grads_list)];
        }
    }

    grads
}
```

---

## Connecting to Session 3: The Opposite Category

In Session 3, we built `OppositeCategory`:

```rust
struct OppositeCategory<C> {
    inner: C,
}

impl<C: Category> Category for OppositeCategory<C> {
    // Morphisms go in reverse direction
    fn compose(&self, f: Morphism, g: Morphism) -> Morphism {
        // f: B → A, g: C → B in Op(C)
        // means f: A → B, g: B → C in C
        // compose in C gives g;f: A → C
        // which is f;g: C → A in Op(C)
        self.inner.compose(g, f)  // Swap order!
    }
}
```

**The backward pass IS this opposite category!**

```rust
/// The backward pass as a functor to the opposite category
pub fn backward_functor(
    forward: &Diagram<DiffOp>
) -> Diagram<VjpOp> {
    // For each forward operation f: A → B
    // Create backward operation vjp(f): B → A

    // For composition f ; g in forward
    // Create vjp(g) ; vjp(f) in backward

    // The diagram structure is "reversed"
}
```

---

## Why This Matters

The functorial view explains:

1. **Why backprop works**: It's just the chain rule, organized by category theory
2. **Why order reverses**: Functors to opposite categories reverse morphisms
3. **Why gradients accumulate**: Multiple paths = sum (coproduct in the opposite category)
4. **Why it composes**: Functors preserve composition

---

## Numerical Gradient Checking

To verify our VJPs are correct:

```rust
fn numerical_gradient(
    f: impl Fn(&[RTensor]) -> RTensor,
    inputs: &[RTensor],
    input_idx: usize,
    elem_idx: usize,
    h: f32,
) -> f32 {
    let mut inputs_plus = inputs.to_vec();
    let mut inputs_minus = inputs.to_vec();

    inputs_plus[input_idx].data[elem_idx] += h;
    inputs_minus[input_idx].data[elem_idx] -= h;

    let f_plus = f(&inputs_plus).as_scalar();
    let f_minus = f(&inputs_minus).as_scalar();

    (f_plus - f_minus) / (2.0 * h)
}

fn grad_check(
    graph: &DiffGraph,
    inputs: &[RTensor],
    analytic_grads: &[RTensor],
    h: f32,
    tolerance: f32,
) -> bool {
    for (input_idx, analytic) in analytic_grads.iter().enumerate() {
        for elem_idx in 0..analytic.data.len() {
            let numerical = numerical_gradient(
                |ins| graph.forward(ins)[0].clone(),
                inputs,
                input_idx,
                elem_idx,
                h,
            );
            let analytic_val = analytic.data[elem_idx];

            if (numerical - analytic_val).abs() > tolerance {
                return false;
            }
        }
    }
    true
}
```

---

## Implementation Sketch

```rust
impl DiffOp {
    /// Compute VJP: given output gradients, return input gradients.
    pub fn vjp(
        &self,
        inputs: &[RTensor],
        output_grads: &[RTensor],
    ) -> Vec<RTensor> {
        match self {
            DiffOp::Add => {
                // grad flows to both inputs
                let grad = &output_grads[0];
                vec![grad.clone(), grad.clone()]
            }

            DiffOp::Mul => {
                let (x, y) = (&inputs[0], &inputs[1]);
                let grad = &output_grads[0];
                vec![grad.mul(y), grad.mul(x)]
            }

            DiffOp::ReLU => {
                let x = &inputs[0];
                let grad = &output_grads[0];
                let mask = x.map(|v| if v > 0.0 { 1.0 } else { 0.0 });
                vec![grad.mul(&mask)]
            }

            DiffOp::MatMul => {
                let (a, b) = (&inputs[0], &inputs[1]);
                let grad_c = &output_grads[0];
                let grad_a = grad_c.matmul(&b.transpose());
                let grad_b = a.transpose().matmul(grad_c);
                vec![grad_a, grad_b]
            }

            DiffOp::SumAll => {
                let x = &inputs[0];
                let grad = output_grads[0].as_scalar();
                vec![RTensor::full(x.shape.clone(), grad)]
            }

            DiffOp::Copy { n_outputs } => {
                // Sum all output gradients
                let mut sum = output_grads[0].clone();
                for g in &output_grads[1..*n_outputs] {
                    sum = sum.add(g);
                }
                vec![sum]
            }

            DiffOp::Input { .. } | DiffOp::Const { .. } => {
                vec![]  // No inputs to propagate to
            }
        }
    }
}
```

---

## Summary

| Concept | Implementation |
|---------|----------------|
| VJP | `op.vjp(inputs, output_grads) → input_grads` |
| Backward pass | Reverse topological order |
| Add VJP | Gradient copies to both inputs |
| Mul VJP | Gradient × other input |
| ReLU VJP | Gradient × (input > 0) |
| MatMul VJP | `grad @ Bᵀ`, `Aᵀ @ grad` |
| Gradient accumulation | Sum gradients from multiple paths |
| Categorical view | Functor to opposite category |

**Key insight**: The backward pass isn't an ad-hoc algorithm—it's the natural structure that arises when you take a functor to the opposite category. The chain rule is composition, and composition reverses in the opposite category.

---

## Next: Session 10

With forward and backward passes working, Session 10 adds:
- **Parameters**: Learnable weights with gradient storage
- **SGD**: Gradient descent optimization loop
- **Training**: Put it all together to train a model
