# Session 12: Bayesian Networks as Composed Kernels

*Reading: Any short Bayes net factorization overview; Fong/Spivak on compositional probability*

---

## The Core Insight

A **Bayesian network** is just a composition of Markov kernels arranged according to a directed acyclic graph (DAG). Each node represents a random variable, each edge represents a conditional dependency, and the joint distribution factors as a product of conditionals.

```text
Bayesian Network:           Categorical View:

    A                       Kernel: 1 → A
    ↓                         ↓
    B ← C                   Kernel: A × C → B
    ↓                         ↓
    D                       Kernel: B → D
```

The joint distribution P(A, B, C, D) = P(A) · P(C) · P(B|A,C) · P(D|B) is a composition of kernels.

---

## Factorization of Joint Distributions

### The Chain Rule

Any joint distribution can be written as:

```text
P(X₁, X₂, ..., Xₙ) = P(X₁) · P(X₂|X₁) · P(X₃|X₁,X₂) · ... · P(Xₙ|X₁,...,Xₙ₋₁)
```

But this requires conditioning on *all* previous variables. Bayesian networks exploit **conditional independence** to simplify.

### Bayesian Network Factorization

If the DAG has parents(Xᵢ) for each node, then:

```text
P(X₁, ..., Xₙ) = ∏ᵢ P(Xᵢ | parents(Xᵢ))
```

Each factor P(Xᵢ | parents(Xᵢ)) is a Markov kernel!

---

## Kernels as Conditional Distributions

A conditional distribution P(Y|X) is exactly a Markov kernel K: X → Y.

```rust
use compositional_prob::{Kernel, Dist};

// P(Rain | Season) where Season ∈ {Summer=0, Winter=1}
// Rain ∈ {No=0, Yes=1}
let p_rain_given_season = Kernel::new(vec![
    vec![0.8, 0.2],  // P(Rain | Summer): 20% chance
    vec![0.3, 0.7],  // P(Rain | Winter): 70% chance
]).unwrap();

// A prior on Season
let p_season = Dist::new(vec![0.5, 0.5]).unwrap();  // Equal seasons

// Joint: P(Season, Rain) via composition
// P(Rain) = Σₛ P(Season=s) · P(Rain|Season=s)
let p_rain = p_rain_given_season.apply(&p_season).unwrap();
// p_rain = [0.55, 0.45]  (55% no rain, 45% rain)
```

---

## Building a Bayesian Network

### Example: Sprinkler Network

The classic example:

```text
     Cloudy (C)
     ↙     ↘
Sprinkler   Rain
    (S)      (R)
       ↘    ↙
      WetGrass (W)
```

Factorization: P(C, S, R, W) = P(C) · P(S|C) · P(R|C) · P(W|S,R)

```rust
use compositional_prob::{Kernel, Dist};

// All variables are binary: 0 = False, 1 = True

// P(Cloudy) - prior
let p_cloudy = Dist::new(vec![0.5, 0.5]).unwrap();

// P(Sprinkler | Cloudy)
let p_sprinkler_given_cloudy = Kernel::new(vec![
    vec![0.5, 0.5],  // Not cloudy: 50% sprinkler
    vec![0.9, 0.1],  // Cloudy: 10% sprinkler (less likely)
]).unwrap();

// P(Rain | Cloudy)
let p_rain_given_cloudy = Kernel::new(vec![
    vec![0.8, 0.2],  // Not cloudy: 20% rain
    vec![0.2, 0.8],  // Cloudy: 80% rain
]).unwrap();

// P(WetGrass | Sprinkler, Rain)
// Input states: (S,R) ∈ {(0,0), (0,1), (1,0), (1,1)} = {0,1,2,3}
let p_wet_given_sr = Kernel::new(vec![
    vec![1.0, 0.0],   // S=0, R=0: grass dry
    vec![0.2, 0.8],   // S=0, R=1: 80% wet (rain)
    vec![0.1, 0.9],   // S=1, R=0: 90% wet (sprinkler)
    vec![0.01, 0.99], // S=1, R=1: 99% wet (both)
]).unwrap();
```

---

## Composition as Wiring

### The Categorical View

Each conditional P(Xᵢ|parents(Xᵢ)) is a morphism in FinStoch:

```text
parents(Xᵢ) ──K_i──▶ Xᵢ
```

The Bayesian network is a **string diagram** where:
- Wires are random variables (objects = state spaces)
- Boxes are conditionals (morphisms = kernels)
- Composition is sequential conditioning
- Tensor product is parallel independence

```text
String Diagram for Sprinkler Network:

    1 ──P(C)──▶ C ──┬── P(S|C) ──▶ S ───┐
                    │                   │
                    └── P(R|C) ──▶ R  ──┼── P(W|S,R) ──▶ W
                                        │
                                        └────────────────┘
```

---

## Marginalization

### Summing Out Variables

To get P(W) from P(C,S,R,W), we **marginalize** (sum out) C, S, R:

```text
P(W) = Σ_C Σ_S Σ_R P(C,S,R,W)
     = Σ_C Σ_S Σ_R P(C) · P(S|C) · P(R|C) · P(W|S,R)
```

Marginalization is a **linear operation** on distributions.

```rust
/// Marginalize a joint distribution over the first variable.
///
/// If joint has shape (n × m), returns a distribution over m states.
pub fn marginalize_first(joint: &[Vec<f32>]) -> Dist {
    let m = joint[0].len();
    let mut marginal = vec![0.0; m];

    for row in joint {
        for (j, &p) in row.iter().enumerate() {
            marginal[j] += p;
        }
    }

    Dist::new(marginal).unwrap()
}

/// Marginalize out the second variable.
///
/// If joint has shape (n × m), returns a distribution over n states.
pub fn marginalize_second(joint: &[Vec<f32>]) -> Dist {
    let marginal: Vec<f32> = joint
        .iter()
        .map(|row| row.iter().sum())
        .collect();

    Dist::new(marginal).unwrap()
}
```

### Marginalization as Composition

In categorical terms, marginalization is composition with the **discard map**:

```text
discard: X → 1    (the unique map to the terminal object)
```

To marginalize Y out of P(X,Y):

```text
P(X,Y) ──(id_X ⊗ discard_Y)──▶ P(X)
```

---

## Joint Distribution from Network

### Building the Full Joint

```rust
use compositional_prob::{Kernel, Dist};

/// Compute the full joint P(C,S,R,W) for the sprinkler network.
fn sprinkler_joint() -> Vec<Vec<Vec<Vec<f32>>>> {
    // P(C)
    let p_c = vec![0.5, 0.5];

    // P(S|C)
    let p_s_given_c = vec![
        vec![0.5, 0.5],  // C=0
        vec![0.9, 0.1],  // C=1
    ];

    // P(R|C)
    let p_r_given_c = vec![
        vec![0.8, 0.2],  // C=0
        vec![0.2, 0.8],  // C=1
    ];

    // P(W|S,R)
    let p_w_given_sr = vec![
        vec![1.0, 0.0],   // S=0,R=0
        vec![0.2, 0.8],   // S=0,R=1
        vec![0.1, 0.9],   // S=1,R=0
        vec![0.01, 0.99], // S=1,R=1
    ];

    // Build joint: P(C,S,R,W) = P(C) · P(S|C) · P(R|C) · P(W|S,R)
    let mut joint = vec![vec![vec![vec![0.0; 2]; 2]; 2]; 2];

    for c in 0..2 {
        for s in 0..2 {
            for r in 0..2 {
                for w in 0..2 {
                    let sr_idx = s * 2 + r;  // Encode (S,R) as index
                    joint[c][s][r][w] =
                        p_c[c] *
                        p_s_given_c[c][s] *
                        p_r_given_c[c][r] *
                        p_w_given_sr[sr_idx][w];
                }
            }
        }
    }

    joint
}

/// Marginalize to get P(W) from the joint.
fn marginal_wet_grass(joint: &Vec<Vec<Vec<Vec<f32>>>>) -> Dist {
    let mut p_w = vec![0.0; 2];

    for c in 0..2 {
        for s in 0..2 {
            for r in 0..2 {
                for w in 0..2 {
                    p_w[w] += joint[c][s][r][w];
                }
            }
        }
    }

    Dist::new(p_w).unwrap()
}
```

---

## Inference as Message Passing

### Variable Elimination

To compute P(W), we can be smarter than building the full joint:

```text
P(W) = Σ_C Σ_S Σ_R P(C) · P(S|C) · P(R|C) · P(W|S,R)
```

Reorder sums to avoid exponential blowup:

```text
P(W) = Σ_S Σ_R P(W|S,R) · [Σ_C P(C) · P(S|C) · P(R|C)]
```

The inner sum Σ_C P(C) · P(S|C) · P(R|C) = P(S,R) can be computed first.

```rust
/// Efficient inference via variable elimination.
fn efficient_marginal_wet() -> Dist {
    // Step 1: Compute P(S,R) by marginalizing out C
    // P(S,R) = Σ_C P(C) · P(S|C) · P(R|C)

    let p_c = vec![0.5, 0.5];
    let p_s_given_c = vec![vec![0.5, 0.5], vec![0.9, 0.1]];
    let p_r_given_c = vec![vec![0.8, 0.2], vec![0.2, 0.8]];

    let mut p_sr = vec![vec![0.0; 2]; 2];  // P(S,R)
    for c in 0..2 {
        for s in 0..2 {
            for r in 0..2 {
                p_sr[s][r] += p_c[c] * p_s_given_c[c][s] * p_r_given_c[c][r];
            }
        }
    }

    // Step 2: Compute P(W) = Σ_S Σ_R P(S,R) · P(W|S,R)
    let p_w_given_sr = vec![
        vec![1.0, 0.0],
        vec![0.2, 0.8],
        vec![0.1, 0.9],
        vec![0.01, 0.99],
    ];

    let mut p_w = vec![0.0; 2];
    for s in 0..2 {
        for r in 0..2 {
            let sr_idx = s * 2 + r;
            for w in 0..2 {
                p_w[w] += p_sr[s][r] * p_w_given_sr[sr_idx][w];
            }
        }
    }

    Dist::new(p_w).unwrap()
}
```

---

## The Compositional Perspective

### Why This Matters

1. **Modularity**: Each conditional is a separate kernel
2. **Reusability**: Same kernel can appear in different networks
3. **Compositionality**: Complex networks = compositions of simple parts
4. **Semantics**: Category theory gives precise meaning to operations

### String Diagrams for Inference

Marginalization in string diagrams:

```text
Before (joint):           After (marginal):

A ──┐                     A ──────────▶ out
    ├── f ──▶ B              (discard)
C ──┘
```

The discard operation "caps off" a wire, summing over that variable.

---

## Conditional Independence

### D-Separation

Two variables X and Y are **conditionally independent** given Z if:

```text
P(X,Y|Z) = P(X|Z) · P(Y|Z)
```

In a DAG, this is determined by **d-separation**: paths between X and Y are "blocked" by Z.

### Categorical View

Conditional independence = factorization through a tensor product:

```text
If X ⊥ Y | Z, then:

Z ──┬── K₁ ──▶ X
    │
    └── K₂ ──▶ Y

is equivalent to:

Z ──(K₁ ⊗ K₂)──▶ X × Y
```

---

## Example: Naive Bayes

### Structure

```text
       Class (C)
      ↙  ↓  ↘
    F₁  F₂  F₃
```

Features are conditionally independent given the class:

```text
P(C, F₁, F₂, F₃) = P(C) · P(F₁|C) · P(F₂|C) · P(F₃|C)
```

### As Kernels

```rust
use compositional_prob::{Kernel, Dist};

// Spam classifier: C ∈ {Ham=0, Spam=1}
// Features: presence of words {money, free, meeting}

let p_class = Dist::new(vec![0.7, 0.3]).unwrap();  // 30% spam

// P(word | class) for each word
let p_money = Kernel::new(vec![
    vec![0.95, 0.05],  // Ham: 5% contains "money"
    vec![0.2, 0.8],    // Spam: 80% contains "money"
]).unwrap();

let p_free = Kernel::new(vec![
    vec![0.98, 0.02],  // Ham: 2% contains "free"
    vec![0.1, 0.9],    // Spam: 90% contains "free"
]).unwrap();

let p_meeting = Kernel::new(vec![
    vec![0.5, 0.5],    // Ham: 50% contains "meeting"
    vec![0.8, 0.2],    // Spam: 20% contains "meeting"
]).unwrap();

// Given features, classify using Bayes' rule
// P(C|F₁,F₂,F₃) ∝ P(C) · P(F₁|C) · P(F₂|C) · P(F₃|C)
```

---

## Summary

| Concept | Categorical View |
|---------|-----------------|
| Random variable | Object (finite set) |
| Conditional P(Y\|X) | Morphism X → Y (kernel) |
| Joint distribution | Composition of kernels |
| Independence | Tensor product structure |
| Marginalization | Composition with discard |
| Bayes net | String diagram of kernels |

**Key insight**: Bayesian networks are just string diagrams in the category FinStoch. The DAG structure tells us how to compose the conditionals (kernels), and marginalization is composition with the unique map to the terminal object.

---

## Next: Session 13

With Bayesian networks as composed kernels, Session 13 explores:
- **Hidden Markov Models** as temporal Bayes nets
- **Forward-backward algorithm** as categorical inference
- Connection between HMMs and recurrent structures
